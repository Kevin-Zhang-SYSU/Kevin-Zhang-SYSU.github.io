<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>vLLM-高效管理内存的LLM推理系统</title>
      <link href="/2024/09/14/vLLM/"/>
      <url>/2024/09/14/vLLM/</url>
      
        <content type="html"><![CDATA[<p>LLM在处理大量请求时面临内存使用效率低下的问题，每个请求需占用大量动态变化的内存，导致浪费并限制处理能力。本文提出了PagedAttention算法，受操作系统虚拟内存分页技术启发，开发了vLLM服务系统。vLLM通过灵活共享内存，实现几乎零内存浪费，显著降低了内存需求。测试表明，vLLM处理吞吐量比FasterTransformer和Orca提高2到4倍，且延迟保持相同，尤其在处理长文本、大模型和复杂解码时表现突出。</p><p>论文：<a href="https://dl.acm.org/doi/pdf/10.1145/3600006.3613165">https://dl.acm.org/doi/pdf/10.1145/3600006.3613165</a><br>代码：<a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a><br>文章参考自：<a href="https://mp.weixin.qq.com/s/whsGK2gfVrIDNXTtxUUSOw">https://mp.weixin.qq.com/s/whsGK2gfVrIDNXTtxUUSOw</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>许多云服务公司正在争相提供LLM应用，但运行这些应用的成本非常高，需要大量的硬件加速器如GPU。据估计，处理一次大语言模型的请求，成本是传统关键词查询的 <strong>10</strong> 倍。由于成本如此之高，提升大语言模型的处理效率，从而降低每次请求的费用，变得越来越重要。</p><p>大语言模型（LLM）的核心是一个自回归的Transformer模型。这个模型根据输入的提示和之前生成的词（标记），一个一个地生成新的词。每次请求都需要重复这个复杂的过程，直到模型输出一个终止标记。这种逐字生成的过程需要大量内存，无法充分利用GPU的计算能力，限制了处理请求的效率。为了提高效率，可以将多个请求批量处理。但要做到这一点，需要有效管理每个请求的内存空间。</p><p><img src="/posts_img/vLLM/1.png"></p><p><strong>内存需求计算公式</strong></p><ul><li><p>模型参数</p><p>13B参数 ~ 13G 个参数 *  4个字节（FP 32） 或者 2个bytes （FP16，生产环境一般用FP16）</p></li><li><p>KV Cache</p><p>2 (key and value vectors) * hidden_size  * layer_num *  head_num * batch_size * seq_len * 参数精度（4&#x2F;2字节）</p></li></ul><p>对于更大的模型，和更长的上下文长度，KV Cache的存储需求会显著增加。由于模型的参数是固定的，而激活占用的内存很少，KV缓存的管理方式决定了最大的批处理大小。如果管理不当，KV缓存会显著限制批处理的大小，从而影响LLM的处理效率。</p><p>作者发现现有的LLM服务系统在管理KV缓存内存方面效率不高。主要因为：</p><ul><li><p><strong>内存碎片化</strong></p><p>KV Cache随着模型生成新词而 <strong>动态增长和收缩</strong>，并且它的生命周期和长度是未知的。现有系统为了在连续空间中存储请求的KV缓存，它们会 <strong>预先分配</strong> 一大块内存，按照请求的最大长度（比如2048个词）来分配，但请求的实际长度往往比最大长度短很多，且不能被其他短请求占用，导致严重的内部碎片化。而且，不同请求的预分配大小不同，还会导致 <strong>外部内存碎片化</strong> 。</p><p><img src="/posts_img/vLLM/3.png"></p><p>图中显示了两个请求：最大可能序列长度为2048的请求A和最大序列长度为512的请求B。三个主要的内存浪费来源：</p><ul><li>为未来令牌预留的插槽</li><li>过度配置最大序列长度而导致的内部碎片</li><li>来自内存分配器（如伙伴分配器）的外部碎片</li></ul><p>作者的分析结果显示，现有系统中只有20.4%-38.2%的KV缓存内存实际用于存储标记状态。</p></li><li><p><strong>无法利用内存共享</strong></p><p>LLM服务常常使用高级解码算法，比如并行采样和束搜索，这些算法会为每个请求生成多个输出。在这些场景中，请求包含的多个序列可以部分共享其KV缓存。但现有系统中KV Cache存储在不同的连续空间中，无法实现内存共享。</p></li></ul><p>为了解决上述问题，作者受 <strong>操作系统虚拟内存分页</strong> 启发，提出了PagedAttention，将请求的KV缓存分成多个小块，每个小块包含固定数量的注意力键和值。这些小块不需要存储在连续的空间中，因此可以像操作系统管理虚拟内存那样灵活地管理KV缓存。通过使用相对较小的小块并按需分配，PagedAttention缓解了内部碎片化问题。此外，由于所有小块大小相同，它还消除了外部碎片化问题。它实现了内存共享，使同一请求的不同序列甚至不同请求之间可以在小块级别共享内存。</p><p>作者基于PagedAttention构建了一个高吞吐量的分布式LLM服务引擎vLLM，该引擎实现了KV缓存内存的近乎零浪费，支持各种流行的大语言模型，如GPT、OPT和LLaMA，并且支持超过单个GPU内存容量的模型。评估结果显示，vLLM在各种模型和工作负载下，相比最先进的系统，其LLM服务吞吐量提高了2-4倍。且这种改进在处理较长序列、较大模型和更复杂的解码算法时更加显著。</p><p><img src="/posts_img/vLLM/2.png"></p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="PageAttention算法"><a href="#PageAttention算法" class="headerlink" title="PageAttention算法"></a>PageAttention算法</h2><p>PagedAttention将每个序列的键值（KV）缓存划分为KV块。每个块包含一定数量的令牌的键和值向量。例如，键块𝐾𝑗包含的是第𝑗个块中的键向量，而值块𝑉𝑗包含的是第𝑗个块中的值向量。在注意力计算过程中，PagedAttention内核分别识别和获取不同的KV块。</p><p><img src="/posts_img/vLLM/5.png"></p><p>例如，当处理查询标记为“forth”时，内核将查询向量𝑞𝑖与每个块中的键向量𝐾𝑗相乘，以计算注意力得分𝐴𝑖𝑗。然后，它将这些得分与每个块中的值向量𝑉𝑗相乘，得出最终的注意力输出。</p><p><img src="/posts_img/vLLM/g1.png"></p><h2 id="KV-Cache管理器"><a href="#KV-Cache管理器" class="headerlink" title="KV Cache管理器"></a>KV Cache管理器</h2><p>KV Cache管理器将KV缓存组织成固定大小的KV块，类似于虚拟内存中的页面。一个请求的KV缓存被表示为一系列逻辑KV块，当新的令牌及其KV缓存生成时，从左到右填充。最后一个KV块的未填充位置保留供将来使用。</p><p>在GPU工作器上，block engine分配一块连续的GPU DRAM，并将其划分为物理KV块。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射关系。每个块表条目记录了逻辑块的相应物理块及其填充位置的数量。将逻辑和物理KV块分开使得vLLM可以动态增长KV缓存内存，而无需提前为所有位置预留空间，这消除了现有系统中大部分的内存浪费。</p><h2 id="基本的推理场景"><a href="#基本的推理场景" class="headerlink" title="基本的推理场景"></a>基本的推理场景</h2><p><img src="/posts_img/vLLM/6.png"></p><p>本例中，提示有7个标记，因此vLLM将前两个逻辑KV块（0和1）映射到两个物理KV块（7和1）。</p><p>在prefill步骤中，vLLM使用传统的自注意力算法生成KV缓存和第一个token。然后，vLLM将前4个标记的KV缓存存储在逻辑块0中，后续3个标记存储在逻辑块1中。剩余的位置留待后续自回归生成阶段使用。</p><p>在第1个decode步骤中，vLLM使用PagedAttention算法在物理块7和1上生成新的标记。由于最后一个逻辑块中还有一个空位，新生成的KV缓存被存储在那里，并且更新了块表中的已填充记录。</p><p>在第2个decode步骤中，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在一个新的逻辑块中；vLLM为其分配一个新的物理块（物理块3）并将此映射存储在块表中。</p><p>对于每个decode迭代，vLLM首先选择一组候选序列进行批处理(原理同Orca选择性批处理)，并为新需求的逻辑块分配物理块。vLLM动态地为逻辑块分配新的物理块，将请求的所有内存浪费限制在一个块内，因此可以有效地利用所有内存，</p><p><img src="/posts_img/vLLM/7.png"></p><p>这两个序列的逻辑块被映射到GPU工作器中块引擎预留的不同物理块中。这两个序列的相邻逻辑块在物理GPU内存中不需要连续，物理块的空间可以被两个序列有效地利用。</p><h2 id="不同的推理场景"><a href="#不同的推理场景" class="headerlink" title="不同的推理场景"></a>不同的推理场景</h2><h3 id="Parallel-sampling（并行采样）"><a href="#Parallel-sampling（并行采样）" class="headerlink" title="Parallel sampling（并行采样）"></a>Parallel sampling（并行采样）</h3><p>在 LLM 应用中，一个输入提示可能生成多个输出，用户可以从多个候选项中选择最喜欢的结果。一个请求包含多个共享相同输入提示的样本，因此提示的KV缓存也可以共享。</p><p><img src="/posts_img/vLLM/8.png"></p><p>图中展示了两个输出的并行解码示例。为了管理共享，vLLM为每个物理块引入了引用计数，物理块7和1的引用计数都是2。在生成阶段，这两个输出生成不同的输出标记，因此需要单独的KV缓存存储。</p><p>vLLM对需要被多个序列修改的物理块实现了块级的copy-on-write机制（类似于操作系统中的写时复制技术）。当样本A1需要写入其最后一个逻辑块（逻辑块1）时，vLLM检测到相应的物理块（物理块1）的引用计数大于1，于是分配一个新的物理块（物理块3），并指示块引擎将信息从物理块1复制过去，同时将引用计数减为1。接下来，当样本A2写入物理块1时，引用计数已经减少到1，因此A2可以直接将新生成的KV缓存写入物理块1。</p><h3 id="Beam-search（束状搜索）"><a href="#Beam-search（束状搜索）" class="headerlink" title="Beam search（束状搜索）"></a>Beam search（束状搜索）</h3><p>在LLM（大型语言模型）任务如机器翻译中，用户通常期望得到最合适的前𝑘个翻译结果。Beam search是一种常用的解码算法，用于从LLM中生成最可能的输出序列。算法依赖于一个参数𝑘，该参数决定每一步保留的最顶尖的候选序列数量。在解码过程中，Beam search会扩展每个候选序列，考虑所有可能的标记，计算它们各自的概率，并从𝑘 · |𝑉|个候选中保留前𝑘个最可能的序列，其中|𝑉|是词汇表的大小。</p><p><img src="/posts_img/vLLM/9.png"></p><p>与并行解码不同，Beam search不仅可以共享初始提示块，还可以在不同的候选序列之间共享其他块，并且这些共享模式会随着解码过程的推进动态变化，类似于操作系统中由复合分叉创建的进程树。</p><p>图中展示了vLLM如何管理一个𝑘&#x3D;4的Beam search示例中的KV块。在图中虚线之前，每个候选序列已经使用了4个完整的逻辑块。所有Beam候选共享第一个块0（即提示）。候选3从第二块开始与其他候选分开。候选0-2共享前三个块，并在第四块分开。在随后的迭代中，最可能的前4个候选都源自候选1和2。由于原始候选0和3不再是顶尖候选，它们的逻辑块被释放，对应的物理块引用计数减少。vLLM释放所有引用计数为0的物理块（块2、4、5、8）。然后，vLLM分配新的物理块（块9-12）以存储新候选生成的KV缓存。</p><p>现在，所有候选共享块0、1、3；候选0和1共享块6，候选2和3进一步共享块7。以前的LLM服务系统需要在Beam候选之间频繁复制KV缓存，这会带来很大的内存复制开销。例如，在图所示的情况下，虚线之后，候选3需要复制候选2的大部分KV缓存以继续生成。vLLM通过物理块共享显著减少了这种频繁的内存复制开销。在vLLM中，不同Beam候选的大多数块可以共享。只有当新生成的标记位于旧的共享块内时，才会应用写时复制机制，这仅涉及复制一个块的数据。</p><p>总结来说，vLLM通过有效的物理块共享和写时复制机制，大大提高了Beam search解码过程中的内存利用效率，减少了内存复制开销。</p><h3 id="Shared-prefix（共享前缀）"><a href="#Shared-prefix（共享前缀）" class="headerlink" title="Shared prefix（共享前缀）"></a>Shared prefix（共享前缀）</h3><p>许多用户提示可能共享一个前缀，因此LLM服务提供者可以提前存储这个前缀的KV缓存，以减少在前缀上的重复计算。在vLLM中，可以通过为一组预定义的共享前缀保留一组物理块来方便地实现这一点，就像操作系统处理跨进程共享库一样。</p><p><img src="/posts_img/vLLM/10.png"></p><h3 id="混合解码"><a href="#混合解码" class="headerlink" title="混合解码"></a>混合解码</h3><p>基于上面的内存管理方法，vLLM能够同时处理具有不同甚至是混合解码偏好的请求，这是现有系统无法高效完成的。这是因为vLLM通过一个公共映射层隐藏了不同序列之间复杂的内存共享，该映射层将逻辑块转换为物理块。</p><p>LLM及其执行内核只需看到每个序列的一组物理块ID，而不需要处理跨序列的共享模式。与现有系统相比，这种方法拓宽了具有不同采样需求的请求的批处理机会，从而最终提高了系统的整体吞吐量。</p><h2 id="调度与抢占"><a href="#调度与抢占" class="headerlink" title="调度与抢占"></a>调度与抢占</h2><p>当请求流量超过系统容量时，采用先到先服务（FCFS）调度策略，确保公平性并防止饥饿。</p><ol><li><p>驱逐<br>通常，驱逐策略使用启发式方法来预测哪个块将来访问的最远，并驱逐该块。由于一个序列的所有块都是一起访问的，所以我们实施了一种全有或全无的驱逐策略，即要么驱逐序列的所有块，要么一个也不驱逐。</p></li><li><p>恢复<br><strong>交换</strong>：将被驱逐的块复制到 CPU 内存中。当 vLLM 为新令牌耗尽自由物理块时，它选择一组序列进行驱逐，并将它们的 KV 缓存传输到 CPU。一旦抢占了一个序列并驱逐了它的块，vLLM 就停止接受新请求，直到所有抢占的序列完成为止。一旦一个请求完成，其块就会从内存中释放，抢占的序列的块就会被重新引入以继续处理该序列。<br><strong>重新计算</strong>：重新计算的延迟可能低于交换延迟，性能取决于 CPU RAM 和 GPU 内存之间的带宽以及 GPU 的计算能力。为了了解两种方法之间的权衡，作者评估了它们的端到端性能，并对它们的开销进行微基准测试。结果显示，交换在小块较小时产生了过多的开销。这是因为小块通常导致CPU和GPU之间大量的小数据传输，从而限制了有效的PCIe带宽。</p></li></ol><p>相比之下，重新计算的开销在不同的块大小下保持不变，因为重新计算不利用KV块。因此，当块大小较小时，重新计算更有效率，而当块大小较大时，交换更有效率，尽管重新计算的开销从未高于交换的延迟的20%。对于从16到64的中等块大小，这两种方法表现出相当的端到端性能。</p><p><img src="/posts_img/vLLM/11.png"></p><h2 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h2><p><img src="/posts_img/vLLM/4.png"></p><p>vLLM使用的Megatron-LM张量并行策略。每个模型分片处理相同的输入，因此需要相同位置的KV缓存。因此，vLLM在集中式调度器中具有一个单一的KV缓存管理器。不同的GPU工作节点共享该管理器，以及逻辑块到物理块的映射。</p><h1 id="系统实现"><a href="#系统实现" class="headerlink" title="系统实现"></a>系统实现</h1><p>vLLM基于FastAPI的前端和基于GPU的推理引擎。前端扩展了OpenAI API接口。</p><p>vLLM引擎由8.5K行Python代码和2K行C++&#x2F;CUDA代码编写而成。使用Python开发了调度器和块管理器等控制相关组件，同时为PagedAttention等关键操作开发了自定义CUDA核心。在分布式GPU工作节点之间的张量通信中，使用NCCL。</p><h2 id="核心级优化"><a href="#核心级优化" class="headerlink" title="核心级优化"></a>核心级优化</h2><p>由于PagedAttention引入了现有系统不高效支持的内存访问模式，开发了几个GPU Kernel进行优化。</p><ul><li>融合重塑和块写入：在每个Transformer层中，将新的KV缓存分割成块，重塑为针对块读取进行优化的内存布局，然后保存在由块表指定的位置。为了最小化核心启动开销，将它们融合成单个核心。</li><li>融合块读取和注意力：调整了FasterTransformer中的注意力核心，根据块表读取KV缓存并实时执行注意力操作。为了确保合并内存访问，为每个块分配了一个GPU线程束。此外，增加了对请求批处理中可变序列长度的支持。</li><li>融合块复制：通过写时复制机制发出的块复制操作可能在不连续的块上操作。如果使用cudaMemcpyAsync API，这可能导致大量小数据移动的调用。为了减轻开销，实现了将不同块的复制操作批处理到单个核心启动中。</li></ul><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>虚拟内存和分页技术在其他GPU工作负载中的应用：</p><p>虚拟内存和分页在LLM（大型语言模型）推理中的有效性源于其动态内存分配需求（因为输出长度不可预知）及其对GPU内存容量的依赖。然而，这并不适用于所有GPU工作负载。例如，在DNN（深度神经网络）训练中，张量形状通常是静态的，因此可以预先优化内存分配。在非LLM的DNN推理中，内存效率的提升可能不会带来性能提升，因为性能主要受计算能力限制。在这些场景中，vLLM技术可能反而会因为内存重定向和非连续内存块带来的额外开销而降低性能。</p><p>然而，对于具有与LLM推理类似特性的其他工作负载，vLLM技术的应用仍具有潜力。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="通用模型服务系统"><a href="#通用模型服务系统" class="headerlink" title="通用模型服务系统"></a>通用模型服务系统</h2><p>Clipper[11]、TensorFlow Serving[33]、Nexus[45]、InferLine[10]和Clockwork[20]是一些早期的通用模型服务系统。他们研究为单个或多个模型提供服务的批处理、缓存、放置和调度。最近，DVABatch[12]引入了多入口多出口批处理。REEF[21]和Shepherd[61]建议优先抢占。AlpaServe[28]利用模型并行性进行统计复用。然而，这些通用系统未能考虑LLM推理的自回归特性和令牌状态，导致错过了优化机会。</p><h2 id="Transformer专用推理服务系统"><a href="#Transformer专用推理服务系统" class="headerlink" title="Transformer专用推理服务系统"></a>Transformer专用推理服务系统</h2><p>这些系统利用GPU内核优化[1,29,31,56]、高级批处理机制[14,60]、模型并行[1,41,60]和参数共享[64]来实现高效服务。其中，Orca[60]与本文的方法最相关。与Orca相比。Orca[60]中的迭代级调度和vLLM中的PagedAttention是互补的技术：虽然这两个系统都旨在提高GPU利用率，从而提高LLM服务的吞吐量，但Orca是通过调度和交织请求来实现的，这样可以并行处理更多请求，而vLLM是通过提高内存利用率来实现的。通过减少内存碎片和启用共享，vLLM并行批处理更多请求，与Orca相比，速度提高了2-4倍。事实上，Orca中请求的细粒度调度和交织使内存管理更具挑战性，使vLLM中提出的技术变得更加关键。</p><h2 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h2><p>GPU的计算能力和内存容量之间的差距越来越大，导致内存成为训练和推理的瓶颈。交换[23,42,55]、重新计算[7,24]及其组合[40]已被用于减少训练的峰值内存。值得注意的是，FlexGen[46]研究了在GPU内存有限情况下如何通过交换LLM推理的权重和令牌状态，但它不针对在线服务设置。OLLA[48]优化了张量的生存期和位置以减少碎片化，但它不进行细粒度块级管理或在线服务。FlashAttention[13]应用平铺和内核优化来减少注意力计算的峰值内存并降低I&#x2F;O成本。本文介绍了一种在线服务环境下块级内存管理的新思路。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文提出了一种新的注意力算法PagedAttention，该算法允许在非连续的分页内存中存储注意力键和值，并介绍了vLLM，一种通过PagedAttention实现高效内存管理的高吞吐量大语言模型（LLM）服务系统。受操作系统的启发，展示了如何改编诸如虚拟内存和写时复制等成熟技术，以高效管理KV缓存并处理LLM服务中的各种解码算法。实验结果表明，vLLM相比现有的最先进系统实现了2-4倍的吞吐量提升。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Orca-大模型推理系统开山之作</title>
      <link href="/2024/09/14/Orca/"/>
      <url>/2024/09/14/Orca/</url>
      
        <content type="html"><![CDATA[<p>本文提出了一种新的分布式服务系统 ORCA，针对大规模 Transformer 模型的自回归生成任务，解决了现有推理服务系统在多迭代特性任务上表现不佳的问题。ORCA 通过引入 <strong>迭代级调度</strong> 和 <strong>选择性批处理</strong> 两项技术，实现了更灵活高效的调度。实验结果表明，在处理 GPT-3 175B 模型时，ORCA 在保持相同延迟的情况下，吞吐量较 NVIDIA FasterTransformer 提升了 36.9 倍。</p><p>论文：<a href="https://www.usenix.org/conference/osdi22/presentation/yu">ORCA: A Distributed Serving System for Transformer-Based Generative Models</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本文讨论了在服务大规模 Transformer 模型时面临的挑战，特别是用于生成任务的模型，如语言生成、代码生成等。典型的例子包括 GPT-3 这样的模型。随着对模型需求的不断增长，低延迟和高吞吐量成为推理系统的目标，早期通过使用如Triton和FasterTransformer的组合来部署服务，Triton主要负责将多个客户端请求分组到一个批中，而FasterTransformer作为模型推理进行优化的执行引擎从Triton接收该批，并以批处理的方式进行推理过程。这些模型通过一种多次迭代的自回归方式处理文本（即每次生成一个词元），这给现有系统带来了特定的挑战。</p><!-- <p align="center">  <img src="posts_img/Orca/1.png" alt="Description of image" width="700" /></p> --><p><img src="/posts_img/Orca/1.png?50"></p><h1 id="问题陈述"><a href="#问题陈述" class="headerlink" title="问题陈述"></a>问题陈述</h1><p>现有的服务基于 Transformer 的生成模型的系统存在以下几个关键低效之处：</p><ul><li>批调度：一个批次中已完成的请求必须等待整个批次完成，导致延迟增加。</li><li>排队延迟：新请求在当前批次完成之前无法处理。</li><li>多次迭代开销：由于每个序列中的词元必须通过迭代处理，同一模型需要多次调用来完成单个推理请求，这影响了吞吐量。</li></ul><p><img src="/posts_img/Orca/2.png?50"></p><p>注意，在语言模型的训练中不会出现提前完成和延迟加入请求的问题；训练过程通过使用teacher forcing technique在一次迭代中完成对整个批次的处理。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>作者提出了 ORCA，一个优化大规模 Transformer 模型的分布式服务系统。ORCA 引入了两个关键技术：</p><h2 id="迭代级调度"><a href="#迭代级调度" class="headerlink" title="迭代级调度"></a>迭代级调度</h2><p>与其对整个请求进行批处理，ORCA 对每次迭代进行调度：</p><p><img src="/posts_img/Orca/3.png?50"></p><ul><li>更快返回已完成的请求，减少延迟。</li><li>在当前迭代后立即处理新到的请求，允许更快的响应。</li></ul><h2 id="选择性批处理"><a href="#选择性批处理" class="headerlink" title="选择性批处理"></a>选择性批处理</h2><p>针对上述所提到的迭代级调度的方法，在一个批中的请求有以下情况：(1) 所处阶段不同，prefill &#x2F; decode。（2）序列长度不同。调度器希望同时存在这两个符合批处理条件的请求才能执行批处理，随着批处理大小的增加，这种可能性进一步呈指数级下降，因此使用大批处理大小来提高吞吐量而不影响延迟是困难的。</p><p><img src="/posts_img/Orca/4.png?100x"></p><p>针对此问题，作者提出的选择性批处理方法仅将批处理应用于特定操作（例如矩阵乘法），而对于无法批处理的操作（如注意力机制，输入张量大小可变的操作）则单独处理，这种选择性方法提升了效率，而不会增加额外的计算成本。</p><h1 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h1><p>本文中调度算法考虑的因素：</p><ul><li>FCFS</li><li>在内存限制的情况下，尽可能地增大batch_size以增大吞吐量</li></ul><p><img src="/posts_img/Orca/7.png?50"></p><p>通过流水线并行，迭代级调度可以消除bubble的影响</p><p><img src="/posts_img/Orca/8.png?50"></p><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>ORCA 使用了 <strong>层内并行</strong> 和 <strong>层间并行</strong> 模型来将大规模模型分布在多个 GPU 上。通过采用 GPU 分区，该系统可以扩展到拥有数百亿参数的 Transformer 模型（如 GPT-3 175B 及更大）。</p><table><thead><tr><th align="center"><img src="/posts_img/Orca/5.png" alt="Image 1"></th><th align="center"><img src="/posts_img/Orca/6.png" alt="Image 2"></th></tr></thead></table><p>该架构主要包括：</p><ul><li><strong>调度器</strong>：根据调度算法选择批。</li><li><strong>引擎主控</strong>：管理迭代级调度，并将任务分发到 GPU。</li><li><strong>工作单元</strong>：每个工作单元负责模型的一部分，可以在多个 GPU 之间处理请求。</li><li><strong>注意力 K&#x2F;V 管理器</strong>：管理和维护跨迭代的注意力键值对，允许其他操作进行选择性批处理。</li></ul><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><h2 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h2><p>ORCA 与 NVIDIA 的 FasterTransformer 进行了对比测试，测试的模型规模达到了 3410 亿参数（如 GPT-3）。实验结果显示：</p><ul><li>吞吐量提升 <strong>36.9</strong> 倍，并且延迟保持相同。</li><li>由于迭代级调度，<strong>排队时间</strong> 显著减少，从而加快响应速度。</li><li>在不同的批处理规模和工作负载下，ORCA 显著优于传统系统。</li></ul><p><img src="/posts_img/Orca/9.png?50"></p><h1 id="先前工作和讨论"><a href="#先前工作和讨论" class="headerlink" title="先前工作和讨论"></a>先前工作和讨论</h1><h2 id="BatchMaker-系统"><a href="#BatchMaker-系统" class="headerlink" title="BatchMaker 系统"></a>BatchMaker 系统</h2><p>BatchMaker 是一个用于 RNN（循环神经网络）的服务系统，能够以 RNN 单元的粒度进行调度和批处理。由于 RNN 每个单元执行相同的计算，因此可以将相同的单元进行批处理，即使它们处于不同的位置（即不同的词元索引）。这种灵活性允许新到达的请求加入当前正在执行的批次，而不需要等待整个批次完成。</p><p>然而，BatchMaker 无法对 Transformer 模型进行同样的批处理，因为 Transformer 中每个单元（如注意力机制中的键值对）随词元索引变化而不同。因此，BatchMaker 在处理不同词元时无法实现批处理，导致处理大多是串行的，并且无法支持大规模模型的并行训练。</p><h2 id="ORCA-的改进"><a href="#ORCA-的改进" class="headerlink" title="ORCA 的改进"></a>ORCA 的改进</h2><p>相比于 BatchMaker，ORCA 的设计基于最大化每次模型参数读取时的计算效率。这是因为对于大模型来说，从 GPU 全局内存读取参数是影响整体执行时间的主要瓶颈。ORCA 通过迭代级调度和选择性批处理来处理所有已准备好的词元，无论它们是否可以被批处理（如 Attention 操作不能被批处理）。</p><h2 id="针对-Transformer-模型的专用推理引擎"><a href="#针对-Transformer-模型的专用推理引擎" class="headerlink" title="针对 Transformer 模型的专用推理引擎"></a>针对 Transformer 模型的专用推理引擎</h2><p>Transformer 模型的性能促使了专门的推理系统的发展，如 FasterTransformer、LightSeq、TurboTransformers 等。这些系统主要作为现有服务系统（如 Triton 或 TensorFlow Serving）的后端推理引擎，仍采用传统的请求级调度。相比之下，ORCA 通过引入更细粒度的调度机制来提升效率。</p><h2 id="服务系统与推理引擎的接口"><a href="#服务系统与推理引擎的接口" class="headerlink" title="服务系统与推理引擎的接口"></a>服务系统与推理引擎的接口</h2><p>当前的通用服务系统如 Triton 和 Clipper 提供了抽象层来处理客户端请求，并调度底层的推理引擎。虽然这种设计分离了服务层和执行层的实现，但在处理像 GPT 这种多迭代特性的模型时存在局限性。ORCA 通过紧密集成调度器与引擎，简化了迭代级调度和选择性批处理的应用。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>ORCA 在处理 Transformer 模型的多次迭代和自回归特性方面显示出显著的改进。<strong>迭代级调度</strong> 和 <strong>选择性批处理</strong> 策略使 ORCA 能够高效处理大规模生成模型，提升了系统的性能和响应速度。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOE基础介绍</title>
      <link href="/2024/09/13/MOE-Intro/"/>
      <url>/2024/09/13/MOE-Intro/</url>
      
        <content type="html"><![CDATA[<p>MOE重要性：坊间一直流传GPT-4是MoE模型<br>本文主要参考自：<a href="https://huggingface.co/blog/zh/moe">https://huggingface.co/blog/zh/moe</a></p><h1 id="什么是MOE"><a href="#什么是MOE" class="headerlink" title="什么是MOE"></a>什么是MOE</h1><p>基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p><p>● <strong>稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。</p><p>● <strong>门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。</p><p><img src="/posts_img/MOE-Intro/1.png"></p><div style="text-align: center;">  <a href="https://arxiv.org/pdf/1701.06538">Outrageously Large Neural Network 论文中的 MoE layer</a></div><p><img src="/posts_img/MOE-Intro/2.png"></p><div style="text-align: center;">  <a href="https://arxiv.org/pdf/2101.03961">Switch Transformers paper 论文中的 MoE layer</a></div><h1 id="混合专家模型（MoEs）简短总结"><a href="#混合专家模型（MoEs）简短总结" class="headerlink" title="混合专家模型（MoEs）简短总结"></a>混合专家模型（MoEs）简短总结</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>● 与稠密模型相比，<strong>预训练速度更快</strong><br>● 与具有相同参数数量的模型相比，具有更快的 <strong>推理速度</strong><br>● 与具有相同激活参数的稠密模型模型相比，具有更高的 <strong>推理精度</strong></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>● 需要 <strong>大量显存</strong>，因为所有专家系统都需要加载到内存中（现有offload技术）<br>● 在 <strong>微调方面</strong> 存在诸多挑战，但 <a href="https://arxiv.org/pdf/2305.14705">近期的研究</a> 表明，对混合专家模型进行指令调优具有很大的潜力<br>● 不同的专家倾向于专注于不同的 <strong>语义</strong>，而不是特定 <strong>领域</strong><br>● 由于设备间需要传递数据，网络带宽常常成为性能瓶颈，应采取适当的并行化策略（3D并行+专家并行）</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)<br>[2] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022)</p>]]></content>
      
      
      <categories>
          
          <category> Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> MOE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOE利用Offload进行高效推理</title>
      <link href="/2024/09/12/MOE-Offloading/"/>
      <url>/2024/09/12/MOE-Offloading/</url>
      
        <content type="html"><![CDATA[<p>这篇文章提出了如何在资源受限的消费级硬件上高效地运行稀疏专家混合（MoE）语言模型的方法。将Mixtral-8x7B这个需要100G以上算力才能部署的模型在12G显存+11G内存的组合下跑出来。<br>论文：<a href="https://arxiv.org/abs/2312.17238">https://arxiv.org/abs/2312.17238</a><br>Colab代码：<a href="https://colab.research.google.com/drive/1ZkC0k487oBEF19R8_9nq2MSHFyQ6OspG?usp=drive_link">https://colab.research.google.com/drive/1ZkC0k487oBEF19R8_9nq2MSHFyQ6OspG?usp=drive_link</a></p><h1 id="引言与背景"><a href="#引言与背景" class="headerlink" title="引言与背景"></a>引言与背景</h1><p>论文的引言部分介绍了大规模预训练语言模型（LLMs）在自然语言处理领域的重要性。这些模型如GPT-3、GPT-4以及其他开放访问的LLMs（如LLaMA、Falcon、BLOOM等）推动了语言技术的迅猛发展。然而，LLMs 的庞大参数量使得它们的推理成本极高，通常需要高端的GPU设备才能运行，限制了它们在普通硬件上的使用。<br>为了缓解这个问题，稀疏的专家混合（MoE）模型被提出。MoE通过只激活模型中的一部分“专家”来计算每个输入，从而提高了计算效率。然而，MoE模型的规模依然庞大，尤其是在需要多GPU的环境下。因此，如何在消费级硬件上运行这些模型是一个重要的研究问题。</p><h1 id="三大解决策略"><a href="#三大解决策略" class="headerlink" title="三大解决策略"></a>三大解决策略</h1><p>Mixtral-8x7B模型中的总参数为46.7亿，专家构成45.1亿（96.6%），在内存受限的情况下，减少专家切换时GPU与RAM之间的数据传输对MoE模型进行推理很关键。作者主要提出通过LRU缓存（LRU caching）和专家的推测性提前加载（Speculative Expert Loading）来减少GPU与RAM之间的数据传输，从而加速推理过程。关键创新点包括：</p><ol><li><p>LRU缓存专家重用模式：MoE模型在处理连续的token时，某些专家会被频繁地重用。因此，作者设计了一种LRU缓存机制，利用这种专家重用的规律来减少GPU-RAM之间的通信开销。<br><img src="/posts_img/MOE-Offloading/1.png"></p></li><li><p>推测性专家加载：由于推理过程中无法提前确定下一层需要加载的专家，因此作者提出了一种基于推测的加载机制，通过对前一层的隐藏状态应用下一层的门控函数来猜测即将需要的专家（可能是因为有residual的原因）。这种机制在推测正确时，下一层的计算可以立即开始，显著减少了推理延迟。</p></li><li><p>混合量化技术：在模型压缩方面，作者使用了一种半二次量化（Half Quadratic Quantization, HQQ）的方法对专家层进行更高的压缩，同时保持其他层的较高精度。这种量化策略有效减少了模型大小，并保持了较好的推理性能。</p></li></ol><h1 id="实验与评估"><a href="#实验与评估" class="headerlink" title="实验与评估"></a>实验与评估</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>用到的模型：Mixtral 8x7B ，一个主流的MOE模型，在大多数基准测试中优于或等价于Llama2 70B, GPT3.5，且推理速度比Llama2 70B快六倍！<br>Mixtral 8x7B 是decoder-only model, 其中 FFN 从8个不同的参数组（专家）中进行挑选，在每一层，每个token, router network 都会选2个组来进行生成并进行组合：</p><ol><li>支持32K上下文</li><li>支持英语，法语，意大利语，德语，西班牙语（中文支持很差）</li><li>在代码生成上很强</li><li>能被微调成一个高分（MT-Bench）的 instruction-following model</li></ol><h2 id="评估结论"><a href="#评估结论" class="headerlink" title="评估结论"></a>评估结论</h2><p>论文在不同硬件配置（如RTX 3060、T4等）下对提出的方法进行了详尽的实验评估，得出了以下几个主要结论：</p><ol><li><p>专家缓存与推测加载的有效性：通过测试不同缓存大小和提前加载的专家数量，作者发现缓存命中率和推测加载的准确率显著提高了模型的推理速度。例如，在缓存大小为4时，缓存命中率可以达到约0.8，推测加载大小为2时，推测加载的准确率则可达到0.9以上。<br><img src="/posts_img/MOE-Offloading/2.png"></p></li><li><p>量化对模型性能的影响：通过对模型进行不同量化比特的测试，作者验证了在保持较好准确率的同时，量化可以有效减少模型大小。例如，使用2-bit量化时，模型的推理延迟显著降低，同时在WikiText2和C4数据集上的困惑度仅略有上升。</p></li><li><p>实际推理性能：在使用完整的算法时，消费级硬件上如RTX 3060和T4可以达到每秒生成2-3个token的性能，远远优于直接在设备内存不足的情况下推理时的性能表现。<br><img src="/posts_img/MOE-Offloading/3.png"></p></li></ol><h1 id="结论与未来工作"><a href="#结论与未来工作" class="headerlink" title="结论与未来工作"></a>结论与未来工作</h1><p>论文总结了该方法在推理速度上相较于传统的加载方式有显著提高，尤其是在消费级硬件和免费云平台（如Google Colab）上，使得大规模稀疏MoE模型的使用更加广泛化。未来的研究方向可能包括进一步优化专家预测加载算法，探索其他的推理加速方法。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇论文解决了大规模稀疏专家模型在推理时的硬件瓶颈问题，提出了一种通过专家缓存与预测加载来优化推理速度的方案，并使用混合量化技术在保证准确率的同时大幅减少了模型大小和推理时间。对于希望在低端硬件上使用大规模语言模型的研究人员来说，本文的贡献提供了一个具有实用价值的解决方案。</p><h1 id="Ideas"><a href="#Ideas" class="headerlink" title="Ideas"></a>Ideas</h1><ol><li>“Note that out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%).” 专家参数占了主导位置，这种异构型能否用于边缘-云端计算，隐私保护等</li><li>对于多用户多对话的在线推理服务系统，采取批量、并行的策略合理使用experts参数，增加吞吐、降低延迟等</li><li>将这种offload方法引入到训练过程中，可以显著扩大模型或数据集的规模</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> MOE </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/09/11/hello-world/"/>
      <url>/2024/09/11/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><p>博客搭建参考文章：<a href="https://blog.csdn.net/mjh1667002013/article/details/129290903">【Hexo】Hexo搭建Butterfly主题并快速美化_hexo butterfly-CSDN博客</a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
