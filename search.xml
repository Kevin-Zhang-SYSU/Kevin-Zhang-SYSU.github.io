<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Sia-考虑集群异构性和作业弹性的DL训练系统</title>
      <link href="/2025/01/16/Sia/"/>
      <url>/2025/01/16/Sia/</url>
      
        <content type="html"><![CDATA[<p>Sia 调度器结合<strong>Gavel</strong>和<strong>Pollux</strong>这两篇文章的优势，提出了一种为<strong>异构</strong>深度学习集群的<strong>弹性</strong>资源自适应作业提供高效资源分配的方法，解决了现有调度器在异构性和资源适应性上的不足。Sia 使用<strong>Bootstrapping + 在线优化</strong>的方法，低开销、快速评估作业在不同配置下的性能，接着使用<strong>ILP算法</strong>进行资源分配，能够在大规模集群中高效扩展，并根据集群负载和作业需求动态调整。Sia 是首个支持混合并行作业弹性扩展的集群调度器。广泛的实验表明，Sia 在多个工作负载环境中显著提高了作业完成效率和资源利用率，并且具有良好的扩展性和公平性，能够支持高达 2000 GPU 的集群。</p><p>论文：(SOSP 2023)<a href="https://suhasjs.github.io/files/sia-sosp23.pdf">Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling</a></p><p>代码：<a href="https://github.com/siasosp23/artifacts">https://github.com/siasosp23/artifacts</a></p><h1 id="研究背景及内容"><a href="#研究背景及内容" class="headerlink" title="研究背景及内容"></a>研究背景及内容</h1><p>深度学习模型的训练和推理，对计算资源的要求极为庞大，对于普通企业和用户而言部署的成本巨大，基于此背景，各大企业的深度学习云计算平台应运而生。</p><p><img src="/posts_img/Sia/1.png"></p><p>用户会将深度学习模型部署到云端，进行训练或推理作业，通常会请求一定的计算资源，以保证自己对任务在时间和精度上的的需求。集群调度器决定如何将资源分配给任务，以实现公平调度、最小化任务完成时间(JCT)、提高集群利用率等目标。</p><p><img src="/posts_img/Sia/2.png"></p><p>现有的调度器在优化资源分配、减少作业完成时间 (JCT) 以及提高 GPU 利用率方面存在局限性：</p><p><img src="/posts_img/Sia/3.png"></p><p>在Sia这篇论文中，作者考虑的场景是DL任务的训练调度，的优化目标是最小化作业的JCT。过去有很多工作考虑弹性调度或者GPU资源的异构，但是没有综合两者进行考虑，其中考虑资源异构的SOTA是Gavel，考虑弹性调度的SOTA是Pollux。作者提出 Sia 调度器，结合异构性与弹性，在实际场景中优化集群资源调度。</p><h1 id="研究方法及过程"><a href="#研究方法及过程" class="headerlink" title="研究方法及过程"></a>研究方法及过程</h1><p>Sia特点：</p><ol><li><p>Sia是一个基于抢占式、轮询的调度器。</p></li><li><p>使用低开销的方法来引导（bootstrap）每个新作业的吞吐量模型，这些模型用于评估可能的资源分配。</p></li><li><p>通过引入一种新的调度公式来解决大规模搜索空间的问题，将作业及其配置与 GPU 类型和数量匹配，同时适应集群负载和作业组合的变化。</p></li></ol><p><img src="/posts_img/Sia/4.png" alt="**Sia 中作业的生命周期**"></p><p>作业提交后，它会在每种 GPU 类型上对几个批次大小进行一次profiling分析。获得资源分配后，作业开始进入一个持续优化的周期（步骤 5-8），持续进行直到它在集群中的生命周期结束。<strong>Policy</strong> 用于 ILP 问题求解，会不断优化作业的分配；<strong>Adaptive Executors</strong> 支持动态调整作业运行配置，如batch size； <strong>Goodput Estimator</strong> 提供最新的性能和梯度统计数据，以帮助决策。</p><p>作业在提交后进行快速初始配置建模，并周期性地重新分配资源以实现动态优化。</p><p>我认为，Sia这篇文章主要回答了以下三个问题。</p><h2 id="Q1-异构集群中，作业的性能因-GPU-类型和数量变化而异，如何高效建模？"><a href="#Q1-异构集群中，作业的性能因-GPU-类型和数量变化而异，如何高效建模？" class="headerlink" title="Q1. 异构集群中，作业的性能因 GPU 类型和数量变化而异，如何高效建模？"></a>Q1. 异构集群中，作业的性能因 GPU 类型和数量变化而异，如何高效建模？</h2><p>答：引入Goodput Estimator模块，使用<strong>Bootstrapping + 在线优化</strong>的方法高效建模。</p><p><img src="/posts_img/Sia/5.png"></p><ul><li>初始profiling建模时最大限度减少开销。</li><li>启发式推断未运行 GPU 类型的多 GPU 配置性能。</li><li>在线学习逐步精确吞吐量模型。</li></ul><h2 id="Q2-作业的动态弹性扩展（GPU-数量）带来巨大搜索空间，如何降低调度开销？"><a href="#Q2-作业的动态弹性扩展（GPU-数量）带来巨大搜索空间，如何降低调度开销？" class="headerlink" title="Q2. 作业的动态弹性扩展（GPU 数量）带来巨大搜索空间，如何降低调度开销？"></a>Q2. 作业的动态弹性扩展（GPU 数量）带来巨大搜索空间，如何降低调度开销？</h2><p>答：在Policy算法中构造配置集合，减小搜索空间。</p><p><img src="/posts_img/Sia/6.png"></p><p>Sia 将配置集合 𝐶 分为两部分：</p><ul><li>单节点配置（Single-node set）: 包含所有在单个节点上的资源分配。约束为 GPU 数量必须是 2 的幂，且最多不超过每节点的 GPU 数量 𝑅。如果 𝑅 不是 2 的幂，可以将节点视为多个虚拟节点。</li><li>多节点配置（Multi-node set）: 包含跨多个节点的资源分配。约束为 GPU 数量必须是每节点 GPU 数量 𝑅 的整数倍（确保使用完整节点）。</li></ul><p>Sia 利用子网形状覆盖定理（<a href="/2024/09/19/Alpa/" title="Alpa-自动生成DL&#x2F;LLM模型并行策略">Alpa-自动生成DL&#x2F;LLM模型并行策略</a>这篇文章证明的），确保所有配置的资源分配是有效的，同时避免多个作业共享节点，减少网络接口（NIC）的资源争用。</p><p>与 Pollux 的对比</p><ul><li><strong>Pollux</strong>: 在资源分配中优化整个搜索空间（即 GPU 数量 × GPU 放置组合），其复杂度为 O(N^R)，其中 N 是节点数，R 是每节点 GPU 数量。</li><li><strong>Sia</strong>: 限制配置集合大小，单节点配置数量为 log₂ R，多节点配置数量为 N，因此总体复杂度为 N + log₂ R。通过限制搜索空间，显著减少了问题复杂度，同时性能与 Pollux 相当。</li></ul><h2 id="Q3-如何设计调度算法提高集群效率（作业完成时间）？"><a href="#Q3-如何设计调度算法提高集群效率（作业完成时间）？" class="headerlink" title="Q3. 如何设计调度算法提高集群效率（作业完成时间）？"></a>Q3. 如何设计调度算法提高集群效率（作业完成时间）？</h2><p>答：<strong>有效吞吐量评估 + 整数线性规划</strong>。</p><p>Sia 使用 goodput 来衡量作业在特定配置下的效率，使得作业在不同 GPU 类型和数量上的效能具有可比性。</p><p><img src="/posts_img/Sia/7.png"></p><p>Goodput根据吞吐量和统计效率得到，用来衡量作业每秒的进度。定义详见Pollux这篇文章。</p><h3 id="Goodput-矩阵-G"><a href="#Goodput-矩阵-G" class="headerlink" title="Goodput 矩阵 G"></a>Goodput 矩阵 G</h3><p>基于最小 goodput 值归一化 Goodput 矩阵：行内值可直接比较，反映每个作业在不同配置下的效用。列间值也可比较，用于评估配置对不同作业的优先级。</p><p><img src="/posts_img/Sia/8.png"></p><p>Sia 为每个作业的每种配置计算出相应的 goodput ，并通过归一化的方式使得其既能根据给定作业选择最适合的配置，又能根据给定配置选择最适合的作业。</p><p>Sia 会一直维护这个矩阵，当新的作业到来时，会在矩阵中添加新的一行。旧的作业完成时，会删除其相应的行。使得 goodput 矩阵一直保持在最新状态，仅适用于活动中的作业。</p><h3 id="整数线性规划（ILP）"><a href="#整数线性规划（ILP）" class="headerlink" title="整数线性规划（ILP）"></a>整数线性规划（ILP）</h3><p><img src="/posts_img/Sia/9.png"></p><ul><li><strong>动态性</strong>: 矩阵 G 实时更新，随着作业的统计效率变化或模型改进不断优化分配决策。</li><li><strong>资源高效利用</strong>: 优化目标结合 goodput 和作业等待惩罚，确保资源高效分配。</li><li><strong>扩展性</strong>: 使用 ILP 求解，能够快速计算大规模集群的优化分配方案。</li></ul><p>基于这个ILP，论文中还提到了许多优化方案：</p><ul><li>重启因子 (Restart Factor)</li><li>公平性调节 (Balancing Goodput and Fairness)</li><li>混合并行训练 (Hybrid-parallel training)</li><li>抢占和预留机制 (Preemption and reservation)</li><li>其他非GPU工作负载的调度 (Other types of workloads)</li><li>…</li></ul><h1 id="实验与分析"><a href="#实验与分析" class="headerlink" title="实验与分析"></a>实验与分析</h1><h2 id="调度中的自适应表现"><a href="#调度中的自适应表现" class="headerlink" title="调度中的自适应表现"></a>调度中的自适应表现</h2><p><img src="/posts_img/Sia/10.png"></p><ul><li>Sia具有良好的自适应性，能够动态调整分配给每个作业的GPU数量和类型。</li><li>Bootstrapping确实能够快速地为作业匹配GPU类型。</li></ul><p>矩形之间的空格是Sia调度的延迟</p><h2 id="调度性能实验"><a href="#调度性能实验" class="headerlink" title="调度性能实验"></a>调度性能实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><h4 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h4><ul><li>异构 GPU 集群，包括 T4、V100 和 A100 等不同 GPU 类型。</li><li>集群规模从几十个到上千个 GPU 节点。</li></ul><h4 id="工作负载"><a href="#工作负载" class="headerlink" title="工作负载"></a>工作负载</h4><p>使用真实生产集群工作负载，包括：</p><ul><li><strong>Philly</strong>：微软 GPU 集群的深度学习工作负载。</li><li><strong>Helios</strong>：基于大规模高负载环境的仿真工作负载。</li><li><strong>newTrace</strong>：实际深度学习训练任务的动态工作负载。</li></ul><h4 id="对比调度器"><a href="#对比调度器" class="headerlink" title="对比调度器"></a>对比调度器</h4><ul><li><strong>Pollux</strong>：专注于作业弹性扩展的调度器。</li><li><strong>Gavel</strong>：优化异构资源分配的调度器。</li></ul><p><img src="/posts_img/Sia/11.png"></p><p><strong>平均 JCT 改善</strong></p><ul><li>在 Philly 数据集中，平均 JCT 比 Pollux 和 Gavel 分别减少 30%-93%。</li><li>在 Helios 数据集中，99 分位数 JCT 减少了 50%-80%。</li></ul><p><img src="/posts_img/Sia/12.png"></p><p><strong>GPU 小时数节省</strong></p><ul><li>在 Helios 工作负载中，GPU 小时数减少 12%-60%。</li></ul><h2 id="算法效率实验"><a href="#算法效率实验" class="headerlink" title="算法效率实验"></a>算法效率实验</h2><p><img src="/posts_img/Sia/13.png"></p><p>Sia具有良好的扩展性，Pollux的遗传算法运行速度明显较慢（比Sia的ILP公式慢100倍），Gavel要快得多，因为它没有考虑工作适应。</p><h1 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h1><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p><strong>联合优化异构性与作业弹性</strong></p><p>支持弹性扩展和异构 GPU 混合调度。</p></li><li><p><strong>动态吞吐量建模</strong></p><p>引入轻量级的在线学习机制，通过少量配置样本快速预测作业性能。快速适应作业需求和资源需求，有效支持异构 GPU 类型和动态负载。</p></li><li><p><strong>高效集群、扩展性与适配性</strong></p><p>支持大规模集群（上千 GPU）的高效调度，支持多种任务类型，任务并行方式、平衡公平性与效率。</p></li></ol><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><ol><li>支持更复杂的混合并行任务调度（如流水线并行与数据并行结合）。</li><li>在超大规模集群（2000+ GPU）中进一步验证性能。</li><li>优化对其他类型工作负载（如实时推理任务）的支持。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Train </tag>
            
            <tag> Mlsys </tag>
            
            <tag> Heterogeneity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Alpa-自动生成DL/LLM模型并行策略</title>
      <link href="/2024/09/19/Alpa/"/>
      <url>/2024/09/19/Alpa/</url>
      
        <content type="html"><![CDATA[<p>只要输入DL模型 computation graph 和 device cluster，Alpa 通过生成统一<strong>数据</strong>、<strong>运算</strong>和<strong>流水线</strong>并行性的执行计划，在<strong>可接受</strong>的时间内<strong>自动化</strong>了大型深度学习（DL）模型的模型并行训练。</p><p>现有的模型并行训练系统要么要求用户手动创建并行化计划，要么从有限的模型并行配置空间中自动生成一个。它们不足以在分布式计算设备上扩展复杂的DL模型。Alpa通过将并行性视为两个层次来分配大型DL模型的训练：**运算内并行性(inter-operator)<strong>和</strong>运算间并行性(intra-operator)**。</p><p>基于此，Alpa为大规模模型并行执行计划构建了一个新的层次空间。Alpa设计了许多编译过程，以在每个并行级别自动导出高效的并行执行计划。Alpa实现了高效的运行时，以协调分布式计算设备上的两级并行执行。评估表明，Alpa生成的并行化计划与手动调优的模型并行训练系统相匹配或优于后者，即使在它们设计的模型上也是如此。与专用系统不同，Alpa还可以推广到具有异构架构的模型和没有手动设计计划的模型。</p><p>论文：(OSDI 2022)<a href="https://www.usenix.org/system/files/osdi23-li-zhuohan.pdf">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a></p><p>源代码：<a href="https://github.com/alpa-projects/alpa">https://github.com/alpa-projects/alpa</a>.</p><p>文章参考自：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/487588274">https://zhuanlan.zhihu.com/p/487588274</a></li><li><a href="https://research.google/blog/alpa-automated-model-parallel-deep-learning/">https://research.google/blog/alpa-automated-model-parallel-deep-learning/</a></li></ol><h1 id="Alpa概念"><a href="#Alpa概念" class="headerlink" title="Alpa概念"></a>Alpa概念</h1><p>作者首先将现有的机器学习并行化策略分为两类：</p><ol><li>运算内并行-Intra-Operator Parallelism：将Tensor按某些维度切裂，放到不同Device上计算的并行方式。如张量并行（如Megatron-LM）、数据并行（如Deepspeed Zero）、专家并行（如GShard MoE）。</li><li>运算间并行-Inter-Operator Parallelism：即流水线并行</li></ol><p>两类并行方式的特点：</p><ol><li>Intra-op Parallelism：通讯量较大，可充分利用带宽，切分带来的通信基本属于高效的集合通信。</li><li>Inter-op Parallelism：只在两个设备间传输数据，通讯量较小，若切点寻找的合适，则通信较小，但同步版本的策略无可避免的会引来Bubble。</li></ol><p>可以利用cluster的非对称特性，将Intra-op Parallelism映射到高带宽互联的devices上；将Inter-op Parallelism映射到低带宽互联的devices上。如此组合，就能释放更大的算力。Alpa会自动探索这些策略及组合情况。</p><p>在GPU集群中，节点内的GPU具有更高的通信带宽，可以适应运算内的并行性。然而，不同节点上的GPU通常以较低的带宽连接（例如以太网），因此首选运算间并行。</p><p>之所以要做这两种区分，目的是为了在不同的level上做策略搜索，然后将二者组合起来，生成大一统的并行方式的执行计划。总结起来就是以下两个点：</p><p>将并行度视为两级：intra-operator和inter-operator，构建分级的解空间<br>在两级空间中<strong>分别</strong>探索最优解，然后提供高效的Runtime将二者编排起来（总体上不能保证全局最优，但是实践证明在大模型上有很强的性能提升）。</p><h1 id="Alpa的Workflow"><a href="#Alpa的Workflow" class="headerlink" title="Alpa的Workflow"></a>Alpa的Workflow</h1><p>Alpa工作在DL编译层（基于XLA，在HLO上进行策略探索），所以文中也称之为——自动生成分布式策略的DL编译器。当用户给出模型计算图和设备集群时，它会进行各种操作流。</p><ol><li>运算间并行操作流-Intra-Operator Pass：传递将计算图切分为子图，将设备集群切片为子表（即分区设备集群），并确定将子图与子表的最佳配对方式。</li><li>运算内并行操作流-Inter-Operator Pass：为运算间并行的每个流水线阶段找到最佳的运算内并行计划。</li><li>运行时编排操作流-Runtime Orchestration pass：生成一个静态计划，对计算和通信进行排序，并在实际设备集群上执行分布式计算图（做runtime缝合的事情。比如stage调度，通信优化等，显然这个pass和策略搜索没什么关系）。</li></ol><p>在宏观上做DP，微观上ILP，Inter-op和Intra-op两个pass不断迭代最终获得最佳方案。</p><p>下图中，在切片子图中，红色和蓝色表示运算的划分方式，灰色表示复制的运算，绿色代表实际设备（例如GPU）：</p><p><img src="/posts_img/Alpa/1.gif" alt="Alpa概述。在切片子图中，红色和蓝色表示运算的划分方式，灰色表示复制的运算，绿色代表实际设备（例如GPU）。"></p><h2 id="运算内并行操作流（Intra-Operator-Pass）"><a href="#运算内并行操作流（Intra-Operator-Pass）" class="headerlink" title="运算内并行操作流（Intra-Operator Pass）"></a>运算内并行操作流（Intra-Operator Pass）</h2><p>与之前的研究（例如Mesh TensorFlow和GSPMD）类似，运算内并行性在设备网格上划分张量。下图显示了Transformer模型中具有给定批次、序列和隐藏维度的典型3D张量。批处理维度沿着设备网格维度0（mesh0）进行划分，隐藏维度沿着网格维度1（mesh1）进行分区，序列维度被复制到每个处理器。</p><p>在2D设备网格上划分的3D张量：<br><img src="/posts_img/Alpa/2.png" alt="在2D设备网格上划分的3D张量"></p><p>通过Alpa中张量的划分，进一步为计算图中的每个独立运算定义了一组并行化策略。在下图中展示了矩阵乘法的并行化策略示例。在运算符上定义并行化策略可能会导致张量分区上的冲突，因为一个张量既可以是一个运算符的输出，也可以是另一个运算的输入。若分区方式不匹配，两个运算之间需要重新分区，这会产生额外的通信成本。</p><p>矩阵乘法的并行化策略：<br><img src="/posts_img/Alpa/3.png" alt="矩阵乘法的并行化策略"></p><p>给定每个运算和re-partition成本，作者将运算内操作流表示为整数线性规划（ILP）问题。对于每个运算，定义一个one-hot向量来枚举分区策略。ILP的目标是最小化计算和通信成本（节点成本）和重新划分通信成本（边成本）的总和。ILP的解决方案转化为一种特定的方法来分割原始计算图。</p><p><img src="/posts_img/Alpa/4.png"></p><h2 id="运算间并行操作流（Inter-Operator-Pass）"><a href="#运算间并行操作流（Inter-Operator-Pass）" class="headerlink" title="运算间并行操作流（Inter-Operator Pass）"></a>运算间并行操作流（Inter-Operator Pass）</h2><p>运算间传递对计算图和设备集群进行切片，以实现流水线并行性。如下图所示，方框表示输入的微批，流水线阶段表示执行子图的子网格。水平维度表示时间，并显示执行微批处理的传递阶段。运算间传递的目标是最大限度地减少总执行延迟，即设备上整个工作负载执行的总和。Alpa使用动态规划（DP）算法来最小化总延迟。计算图首先被展平，然后执行Intra-Operator Pass，对设备集群到子表的所有可能分区的性能进行分析。</p><p>对于给定的时间，此图显示了分区设备集群和计算图切片（例如，阶段1、2、3）正在处理的微批（彩色框）：<br><img src="/posts_img/Alpa/6.png" alt="流水线并行。对于给定的时间，此图显示了分区设备集群和计算图切片（例如，阶段1、2、3）正在处理的微批（彩色框）。"></p><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><p>使用8个AWS p3.16xlarge实例测试Alpa，每个实例有8个16GB V100 GPU，总共64个GPU。研究了在增加GPU数量的同时增加模型大小的弱缩放结果。我们评估了三种模型：</p><ol><li>标准Transformer模型（GPT）；</li><li>GShard MoE模型，一种混合了专家层的Transformer；</li><li>Wide ResNet，一个明显不同的模型，没有现有的专家设计的模型并行化策略。性能是通过集群上每秒实现的peta浮点运算（PFLOPS）来衡量的。</li></ol><p>作者证明，对于GPT，Alpa输出的并行化策略与现有最佳框架Megatron ML计算的策略非常相似，并与其性能相匹配。对于GShard MoE来说，Alpa在GPU（即Deepspeed）上的表现比专家设计的最佳基线高出8倍。Wide ResNet的结果表明，Alpa可以为专家尚未研究的模型生成最佳并行化策略。本文还展示了线性缩放数以供参考。</p><p><img src="/posts_img/Alpa/7.png"></p><p>在16个GPU上，Alpa将模型分为3个阶段，并分别为第1、2、3阶段分配4、4、8个GPU。在前两个阶段，数据并行性是首选，因为激活张量大于权重张量。在第三阶段，ILP求解器找到了一种划分卷积算子的非平凡方法。结果表明，对于像Wide ResNet这样的异构模型，即使对于领域专家来说，手动创建这样的策略也可能是困难的。</p><p>Alpa在16个GPU上为WideResNet找到的并行化策略：<br><img src="/posts_img/Alpa/8.png" alt="Alpa在16个GPU上为WideResNet找到的并行化策略"></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>为分布式模型并行深度学习设计有效的并行化计划的过程历来是一项困难且劳动密集型的任务。Alpa是一个新的框架，它利用运算内和运算间的并行性进行自动化模型并行分布式训练。相信Alpa将使分布式模型并行学习规范化，并加速大型深度学习模型的开发。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> DL </tag>
            
            <tag> Train </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vLLM-高效管理内存的LLM推理系统</title>
      <link href="/2024/09/14/vLLM/"/>
      <url>/2024/09/14/vLLM/</url>
      
        <content type="html"><![CDATA[<p>LLM在处理大量请求时面临内存使用效率低下的问题，每个请求需占用大量动态变化的内存，导致浪费并限制处理能力。本文提出了PagedAttention算法，受操作系统虚拟内存分页技术启发，开发了vLLM服务系统。vLLM通过灵活共享内存，实现几乎零内存浪费，显著降低了内存需求。测试表明，vLLM处理吞吐量比FasterTransformer和Orca提高2到4倍，且延迟保持相同，尤其在处理长文本、大模型和复杂解码时表现突出。</p><p>论文：(SOSP 2023)<img src="https://dl.acm.org/doi/pdf/10.1145/3600006.3613165" alt="Efficient Memory Management for Large Language Model Serving with PagedAttention"><br>代码：<a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a><br>文章参考自：<a href="https://mp.weixin.qq.com/s/whsGK2gfVrIDNXTtxUUSOw">https://mp.weixin.qq.com/s/whsGK2gfVrIDNXTtxUUSOw</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>许多云服务公司正在争相提供LLM应用，但运行这些应用的成本非常高，需要大量的硬件加速器如GPU。据估计，处理一次大语言模型的请求，成本是传统关键词查询的 <strong>10</strong> 倍。由于成本如此之高，提升大语言模型的处理效率，从而降低每次请求的费用，变得越来越重要。</p><p>大语言模型（LLM）的核心是一个自回归的Transformer模型。这个模型根据输入的提示和之前生成的词（标记），一个一个地生成新的词。每次请求都需要重复这个复杂的过程，直到模型输出一个终止标记。这种逐字生成的过程需要大量内存，无法充分利用GPU的计算能力，限制了处理请求的效率。为了提高效率，可以将多个请求批量处理。但要做到这一点，需要有效管理每个请求的内存空间。</p><p><img src="/posts_img/vLLM/1.png"></p><p><strong>内存需求计算公式</strong></p><ul><li><p>模型参数</p><p>13B参数 ~ 13G 个参数 *  4个字节（FP 32） 或者 2个bytes （FP16，生产环境一般用FP16）</p></li><li><p>KV Cache</p><p>2 (key and value vectors) * hidden_size  * layer_num *  head_num * batch_size * seq_len * 参数精度（4&#x2F;2字节）</p></li></ul><p>对于更大的模型，和更长的上下文长度，KV Cache的存储需求会显著增加。由于模型的参数是固定的，而激活占用的内存很少，KV缓存的管理方式决定了最大的批处理大小。如果管理不当，KV缓存会显著限制批处理的大小，从而影响LLM的处理效率。</p><p>作者发现现有的LLM服务系统在管理KV缓存内存方面效率不高。主要因为：</p><ul><li><p><strong>内存碎片化</strong></p><p>KV Cache随着模型生成新词而 <strong>动态增长和收缩</strong>，并且它的生命周期和长度是未知的。现有系统为了在连续空间中存储请求的KV缓存，它们会 <strong>预先分配</strong> 一大块内存，按照请求的最大长度（比如2048个词）来分配，但请求的实际长度往往比最大长度短很多，且不能被其他短请求占用，导致严重的内部碎片化。而且，不同请求的预分配大小不同，还会导致 <strong>外部内存碎片化</strong> 。</p><p><img src="/posts_img/vLLM/3.png"></p><p>图中显示了两个请求：最大可能序列长度为2048的请求A和最大序列长度为512的请求B。三个主要的内存浪费来源：</p><ul><li>为未来令牌预留的插槽</li><li>过度配置最大序列长度而导致的内部碎片</li><li>来自内存分配器（如伙伴分配器）的外部碎片</li></ul><p>作者的分析结果显示，现有系统中只有20.4%-38.2%的KV缓存内存实际用于存储标记状态。</p></li><li><p><strong>无法利用内存共享</strong></p><p>LLM服务常常使用高级解码算法，比如并行采样和束搜索，这些算法会为每个请求生成多个输出。在这些场景中，请求包含的多个序列可以部分共享其KV缓存。但现有系统中KV Cache存储在不同的连续空间中，无法实现内存共享。</p></li></ul><p>为了解决上述问题，作者受 <strong>操作系统虚拟内存分页</strong> 启发，提出了PagedAttention，将请求的KV缓存分成多个小块，每个小块包含固定数量的注意力键和值。这些小块不需要存储在连续的空间中，因此可以像操作系统管理虚拟内存那样灵活地管理KV缓存。通过使用相对较小的小块并按需分配，PagedAttention缓解了内部碎片化问题。此外，由于所有小块大小相同，它还消除了外部碎片化问题。它实现了内存共享，使同一请求的不同序列甚至不同请求之间可以在小块级别共享内存。</p><p>作者基于PagedAttention构建了一个高吞吐量的分布式LLM服务引擎vLLM，该引擎实现了KV缓存内存的近乎零浪费，支持各种流行的大语言模型，如GPT、OPT和LLaMA，并且支持超过单个GPU内存容量的模型。评估结果显示，vLLM在各种模型和工作负载下，相比最先进的系统，其LLM服务吞吐量提高了2-4倍。且这种改进在处理较长序列、较大模型和更复杂的解码算法时更加显著。</p><p><img src="/posts_img/vLLM/2.png"></p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="PageAttention算法"><a href="#PageAttention算法" class="headerlink" title="PageAttention算法"></a>PageAttention算法</h2><p>PagedAttention将每个序列的键值（KV）缓存划分为KV块。每个块包含一定数量的令牌的键和值向量。例如，键块𝐾𝑗包含的是第𝑗个块中的键向量，而值块𝑉𝑗包含的是第𝑗个块中的值向量。在注意力计算过程中，PagedAttention内核分别识别和获取不同的KV块。</p><p><img src="/posts_img/vLLM/5.png"></p><p>例如，当处理查询标记为“forth”时，内核将查询向量𝑞𝑖与每个块中的键向量𝐾𝑗相乘，以计算注意力得分𝐴𝑖𝑗。然后，它将这些得分与每个块中的值向量𝑉𝑗相乘，得出最终的注意力输出。</p><p><img src="/posts_img/vLLM/g1.png"></p><h2 id="KV-Cache管理器"><a href="#KV-Cache管理器" class="headerlink" title="KV Cache管理器"></a>KV Cache管理器</h2><p>KV Cache管理器将KV缓存组织成固定大小的KV块，类似于虚拟内存中的页面。一个请求的KV缓存被表示为一系列逻辑KV块，当新的令牌及其KV缓存生成时，从左到右填充。最后一个KV块的未填充位置保留供将来使用。</p><p>在GPU工作器上，block engine分配一块连续的GPU DRAM，并将其划分为物理KV块。KV块管理器还维护块表——每个请求的逻辑和物理KV块之间的映射关系。每个块表条目记录了逻辑块的相应物理块及其填充位置的数量。将逻辑和物理KV块分开使得vLLM可以动态增长KV缓存内存，而无需提前为所有位置预留空间，这消除了现有系统中大部分的内存浪费。</p><h2 id="基本的推理场景"><a href="#基本的推理场景" class="headerlink" title="基本的推理场景"></a>基本的推理场景</h2><p><img src="/posts_img/vLLM/6.png"></p><p>本例中，提示有7个标记，因此vLLM将前两个逻辑KV块（0和1）映射到两个物理KV块（7和1）。</p><p>在prefill步骤中，vLLM使用传统的自注意力算法生成KV缓存和第一个token。然后，vLLM将前4个标记的KV缓存存储在逻辑块0中，后续3个标记存储在逻辑块1中。剩余的位置留待后续自回归生成阶段使用。</p><p>在第1个decode步骤中，vLLM使用PagedAttention算法在物理块7和1上生成新的标记。由于最后一个逻辑块中还有一个空位，新生成的KV缓存被存储在那里，并且更新了块表中的已填充记录。</p><p>在第2个decode步骤中，由于最后一个逻辑块已满，vLLM将新生成的KV缓存存储在一个新的逻辑块中；vLLM为其分配一个新的物理块（物理块3）并将此映射存储在块表中。</p><p>对于每个decode迭代，vLLM首先选择一组候选序列进行批处理(原理同Orca选择性批处理)，并为新需求的逻辑块分配物理块。vLLM动态地为逻辑块分配新的物理块，将请求的所有内存浪费限制在一个块内，因此可以有效地利用所有内存，</p><p><img src="/posts_img/vLLM/7.png"></p><p>这两个序列的逻辑块被映射到GPU工作器中块引擎预留的不同物理块中。这两个序列的相邻逻辑块在物理GPU内存中不需要连续，物理块的空间可以被两个序列有效地利用。</p><h2 id="不同的推理场景"><a href="#不同的推理场景" class="headerlink" title="不同的推理场景"></a>不同的推理场景</h2><h3 id="Parallel-sampling（并行采样）"><a href="#Parallel-sampling（并行采样）" class="headerlink" title="Parallel sampling（并行采样）"></a>Parallel sampling（并行采样）</h3><p>在 LLM 应用中，一个输入提示可能生成多个输出，用户可以从多个候选项中选择最喜欢的结果。一个请求包含多个共享相同输入提示的样本，因此提示的KV缓存也可以共享。</p><p><img src="/posts_img/vLLM/8.png"></p><p>图中展示了两个输出的并行解码示例。为了管理共享，vLLM为每个物理块引入了引用计数，物理块7和1的引用计数都是2。在生成阶段，这两个输出生成不同的输出标记，因此需要单独的KV缓存存储。</p><p>vLLM对需要被多个序列修改的物理块实现了块级的copy-on-write机制（类似于操作系统中的写时复制技术）。当样本A1需要写入其最后一个逻辑块（逻辑块1）时，vLLM检测到相应的物理块（物理块1）的引用计数大于1，于是分配一个新的物理块（物理块3），并指示块引擎将信息从物理块1复制过去，同时将引用计数减为1。接下来，当样本A2写入物理块1时，引用计数已经减少到1，因此A2可以直接将新生成的KV缓存写入物理块1。</p><h3 id="Beam-search（束状搜索）"><a href="#Beam-search（束状搜索）" class="headerlink" title="Beam search（束状搜索）"></a>Beam search（束状搜索）</h3><p>在LLM（大型语言模型）任务如机器翻译中，用户通常期望得到最合适的前𝑘个翻译结果。Beam search是一种常用的解码算法，用于从LLM中生成最可能的输出序列。算法依赖于一个参数𝑘，该参数决定每一步保留的最顶尖的候选序列数量。在解码过程中，Beam search会扩展每个候选序列，考虑所有可能的标记，计算它们各自的概率，并从𝑘 · |𝑉|个候选中保留前𝑘个最可能的序列，其中|𝑉|是词汇表的大小。</p><p><img src="/posts_img/vLLM/9.png"></p><p>与并行解码不同，Beam search不仅可以共享初始提示块，还可以在不同的候选序列之间共享其他块，并且这些共享模式会随着解码过程的推进动态变化，类似于操作系统中由复合分叉创建的进程树。</p><p>图中展示了vLLM如何管理一个𝑘&#x3D;4的Beam search示例中的KV块。在图中虚线之前，每个候选序列已经使用了4个完整的逻辑块。所有Beam候选共享第一个块0（即提示）。候选3从第二块开始与其他候选分开。候选0-2共享前三个块，并在第四块分开。在随后的迭代中，最可能的前4个候选都源自候选1和2。由于原始候选0和3不再是顶尖候选，它们的逻辑块被释放，对应的物理块引用计数减少。vLLM释放所有引用计数为0的物理块（块2、4、5、8）。然后，vLLM分配新的物理块（块9-12）以存储新候选生成的KV缓存。</p><p>现在，所有候选共享块0、1、3；候选0和1共享块6，候选2和3进一步共享块7。以前的LLM服务系统需要在Beam候选之间频繁复制KV缓存，这会带来很大的内存复制开销。例如，在图所示的情况下，虚线之后，候选3需要复制候选2的大部分KV缓存以继续生成。vLLM通过物理块共享显著减少了这种频繁的内存复制开销。在vLLM中，不同Beam候选的大多数块可以共享。只有当新生成的标记位于旧的共享块内时，才会应用写时复制机制，这仅涉及复制一个块的数据。</p><p>总结来说，vLLM通过有效的物理块共享和写时复制机制，大大提高了Beam search解码过程中的内存利用效率，减少了内存复制开销。</p><h3 id="Shared-prefix（共享前缀）"><a href="#Shared-prefix（共享前缀）" class="headerlink" title="Shared prefix（共享前缀）"></a>Shared prefix（共享前缀）</h3><p>许多用户提示可能共享一个前缀，因此LLM服务提供者可以提前存储这个前缀的KV缓存，以减少在前缀上的重复计算。在vLLM中，可以通过为一组预定义的共享前缀保留一组物理块来方便地实现这一点，就像操作系统处理跨进程共享库一样。</p><p><img src="/posts_img/vLLM/10.png"></p><h3 id="混合解码"><a href="#混合解码" class="headerlink" title="混合解码"></a>混合解码</h3><p>基于上面的内存管理方法，vLLM能够同时处理具有不同甚至是混合解码偏好的请求，这是现有系统无法高效完成的。这是因为vLLM通过一个公共映射层隐藏了不同序列之间复杂的内存共享，该映射层将逻辑块转换为物理块。</p><p>LLM及其执行内核只需看到每个序列的一组物理块ID，而不需要处理跨序列的共享模式。与现有系统相比，这种方法拓宽了具有不同采样需求的请求的批处理机会，从而最终提高了系统的整体吞吐量。</p><h2 id="调度与抢占"><a href="#调度与抢占" class="headerlink" title="调度与抢占"></a>调度与抢占</h2><p>当请求流量超过系统容量时，采用先到先服务（FCFS）调度策略，确保公平性并防止饥饿。</p><ol><li><p>驱逐<br>通常，驱逐策略使用启发式方法来预测哪个块将来访问的最远，并驱逐该块。由于一个序列的所有块都是一起访问的，所以我们实施了一种全有或全无的驱逐策略，即要么驱逐序列的所有块，要么一个也不驱逐。</p></li><li><p>恢复<br><strong>交换</strong>：将被驱逐的块复制到 CPU 内存中。当 vLLM 为新令牌耗尽自由物理块时，它选择一组序列进行驱逐，并将它们的 KV 缓存传输到 CPU。一旦抢占了一个序列并驱逐了它的块，vLLM 就停止接受新请求，直到所有抢占的序列完成为止。一旦一个请求完成，其块就会从内存中释放，抢占的序列的块就会被重新引入以继续处理该序列。<br><strong>重新计算</strong>：重新计算的延迟可能低于交换延迟，性能取决于 CPU RAM 和 GPU 内存之间的带宽以及 GPU 的计算能力。为了了解两种方法之间的权衡，作者评估了它们的端到端性能，并对它们的开销进行微基准测试。结果显示，交换在小块较小时产生了过多的开销。这是因为小块通常导致CPU和GPU之间大量的小数据传输，从而限制了有效的PCIe带宽。</p></li></ol><p>相比之下，重新计算的开销在不同的块大小下保持不变，因为重新计算不利用KV块。因此，当块大小较小时，重新计算更有效率，而当块大小较大时，交换更有效率，尽管重新计算的开销从未高于交换的延迟的20%。对于从16到64的中等块大小，这两种方法表现出相当的端到端性能。</p><p><img src="/posts_img/vLLM/11.png"></p><h2 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h2><p><img src="/posts_img/vLLM/4.png"></p><p>vLLM使用的Megatron-LM张量并行策略。每个模型分片处理相同的输入，因此需要相同位置的KV缓存。因此，vLLM在集中式调度器中具有一个单一的KV缓存管理器。不同的GPU工作节点共享该管理器，以及逻辑块到物理块的映射。</p><h1 id="系统实现"><a href="#系统实现" class="headerlink" title="系统实现"></a>系统实现</h1><p>vLLM基于FastAPI的前端和基于GPU的推理引擎。前端扩展了OpenAI API接口。</p><p>vLLM引擎由8.5K行Python代码和2K行C++&#x2F;CUDA代码编写而成。使用Python开发了调度器和块管理器等控制相关组件，同时为PagedAttention等关键操作开发了自定义CUDA核心。在分布式GPU工作节点之间的张量通信中，使用NCCL。</p><h2 id="核心级优化"><a href="#核心级优化" class="headerlink" title="核心级优化"></a>核心级优化</h2><p>由于PagedAttention引入了现有系统不高效支持的内存访问模式，开发了几个GPU Kernel进行优化。</p><ul><li>融合重塑和块写入：在每个Transformer层中，将新的KV缓存分割成块，重塑为针对块读取进行优化的内存布局，然后保存在由块表指定的位置。为了最小化核心启动开销，将它们融合成单个核心。</li><li>融合块读取和注意力：调整了FasterTransformer中的注意力核心，根据块表读取KV缓存并实时执行注意力操作。为了确保合并内存访问，为每个块分配了一个GPU线程束。此外，增加了对请求批处理中可变序列长度的支持。</li><li>融合块复制：通过写时复制机制发出的块复制操作可能在不连续的块上操作。如果使用cudaMemcpyAsync API，这可能导致大量小数据移动的调用。为了减轻开销，实现了将不同块的复制操作批处理到单个核心启动中。</li></ul><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>虚拟内存和分页技术在其他GPU工作负载中的应用：</p><p>虚拟内存和分页在LLM（大型语言模型）推理中的有效性源于其动态内存分配需求（因为输出长度不可预知）及其对GPU内存容量的依赖。然而，这并不适用于所有GPU工作负载。例如，在DNN（深度神经网络）训练中，张量形状通常是静态的，因此可以预先优化内存分配。在非LLM的DNN推理中，内存效率的提升可能不会带来性能提升，因为性能主要受计算能力限制。在这些场景中，vLLM技术可能反而会因为内存重定向和非连续内存块带来的额外开销而降低性能。</p><p>然而，对于具有与LLM推理类似特性的其他工作负载，vLLM技术的应用仍具有潜力。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="通用模型服务系统"><a href="#通用模型服务系统" class="headerlink" title="通用模型服务系统"></a>通用模型服务系统</h2><p>Clipper[11]、TensorFlow Serving[33]、Nexus[45]、InferLine[10]和Clockwork[20]是一些早期的通用模型服务系统。他们研究为单个或多个模型提供服务的批处理、缓存、放置和调度。最近，DVABatch[12]引入了多入口多出口批处理。REEF[21]和Shepherd[61]建议优先抢占。AlpaServe[28]利用模型并行性进行统计复用。然而，这些通用系统未能考虑LLM推理的自回归特性和令牌状态，导致错过了优化机会。</p><h2 id="Transformer专用推理服务系统"><a href="#Transformer专用推理服务系统" class="headerlink" title="Transformer专用推理服务系统"></a>Transformer专用推理服务系统</h2><p>这些系统利用GPU内核优化[1,29,31,56]、高级批处理机制[14,60]、模型并行[1,41,60]和参数共享[64]来实现高效服务。其中，Orca[60]与本文的方法最相关。与Orca相比。Orca[60]中的迭代级调度和vLLM中的PagedAttention是互补的技术：虽然这两个系统都旨在提高GPU利用率，从而提高LLM服务的吞吐量，但Orca是通过调度和交织请求来实现的，这样可以并行处理更多请求，而vLLM是通过提高内存利用率来实现的。通过减少内存碎片和启用共享，vLLM并行批处理更多请求，与Orca相比，速度提高了2-4倍。事实上，Orca中请求的细粒度调度和交织使内存管理更具挑战性，使vLLM中提出的技术变得更加关键。</p><h2 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h2><p>GPU的计算能力和内存容量之间的差距越来越大，导致内存成为训练和推理的瓶颈。交换[23,42,55]、重新计算[7,24]及其组合[40]已被用于减少训练的峰值内存。值得注意的是，FlexGen[46]研究了在GPU内存有限情况下如何通过交换LLM推理的权重和令牌状态，但它不针对在线服务设置。OLLA[48]优化了张量的生存期和位置以减少碎片化，但它不进行细粒度块级管理或在线服务。FlashAttention[13]应用平铺和内核优化来减少注意力计算的峰值内存并降低I&#x2F;O成本。本文介绍了一种在线服务环境下块级内存管理的新思路。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文提出了一种新的注意力算法PagedAttention，该算法允许在非连续的分页内存中存储注意力键和值，并介绍了vLLM，一种通过PagedAttention实现高效内存管理的高吞吐量大语言模型（LLM）服务系统。受操作系统的启发，展示了如何改编诸如虚拟内存和写时复制等成熟技术，以高效管理KV缓存并处理LLM服务中的各种解码算法。实验结果表明，vLLM相比现有的最先进系统实现了2-4倍的吞吐量提升。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Orca-大模型推理系统开山之作</title>
      <link href="/2024/09/14/Orca/"/>
      <url>/2024/09/14/Orca/</url>
      
        <content type="html"><![CDATA[<p>本文提出了一种新的分布式服务系统 ORCA，针对大规模 Transformer 模型的自回归生成任务，解决了现有推理服务系统在多迭代特性任务上表现不佳的问题。ORCA 通过引入 <strong>迭代级调度</strong> 和 <strong>选择性批处理</strong> 两项技术，实现了更灵活高效的调度。实验结果表明，在处理 GPT-3 175B 模型时，ORCA 在保持相同延迟的情况下，吞吐量较 NVIDIA FasterTransformer 提升了 36.9 倍。</p><p>论文：(OSDI 2022)<a href="https://www.usenix.org/conference/osdi22/presentation/yu">ORCA: A Distributed Serving System for Transformer-Based Generative Models</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本文讨论了在服务大规模 Transformer 模型时面临的挑战，特别是用于生成任务的模型，如语言生成、代码生成等。典型的例子包括 GPT-3 这样的模型。随着对模型需求的不断增长，低延迟和高吞吐量成为推理系统的目标，早期通过使用如Triton和FasterTransformer的组合来部署服务，Triton主要负责将多个客户端请求分组到一个批中，而FasterTransformer作为模型推理进行优化的执行引擎从Triton接收该批，并以批处理的方式进行推理过程。这些模型通过一种多次迭代的自回归方式处理文本（即每次生成一个词元），这给现有系统带来了特定的挑战。</p><!-- <p align="center">  <img src="posts_img/Orca/1.png" alt="Description of image" width="700" /></p> --><p><img src="/posts_img/Orca/1.png?50"></p><h1 id="问题陈述"><a href="#问题陈述" class="headerlink" title="问题陈述"></a>问题陈述</h1><p>现有的服务基于 Transformer 的生成模型的系统存在以下几个关键低效之处：</p><ul><li>批调度：一个批次中已完成的请求必须等待整个批次完成，导致延迟增加。</li><li>排队延迟：新请求在当前批次完成之前无法处理。</li><li>多次迭代开销：由于每个序列中的词元必须通过迭代处理，同一模型需要多次调用来完成单个推理请求，这影响了吞吐量。</li></ul><p><img src="/posts_img/Orca/2.png?50"></p><p>注意，在语言模型的训练中不会出现提前完成和延迟加入请求的问题；训练过程通过使用teacher forcing technique在一次迭代中完成对整个批次的处理。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>作者提出了 ORCA，一个优化大规模 Transformer 模型的分布式服务系统。ORCA 引入了两个关键技术：</p><h2 id="迭代级调度"><a href="#迭代级调度" class="headerlink" title="迭代级调度"></a>迭代级调度</h2><p>与其对整个请求进行批处理，ORCA 对每次迭代进行调度：</p><p><img src="/posts_img/Orca/3.png?50"></p><ul><li>更快返回已完成的请求，减少延迟。</li><li>在当前迭代后立即处理新到的请求，允许更快的响应。</li></ul><h2 id="选择性批处理"><a href="#选择性批处理" class="headerlink" title="选择性批处理"></a>选择性批处理</h2><p>针对上述所提到的迭代级调度的方法，在一个批中的请求有以下情况：(1) 所处阶段不同，prefill &#x2F; decode。（2）序列长度不同。调度器希望同时存在这两个符合批处理条件的请求才能执行批处理，随着批处理大小的增加，这种可能性进一步呈指数级下降，因此使用大批处理大小来提高吞吐量而不影响延迟是困难的。</p><p><img src="/posts_img/Orca/4.png?100x"></p><p>针对此问题，作者提出的选择性批处理方法仅将批处理应用于特定操作（例如矩阵乘法），而对于无法批处理的操作（如注意力机制，输入张量大小可变的操作）则单独处理，这种选择性方法提升了效率，而不会增加额外的计算成本。</p><h1 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h1><p>本文中调度算法考虑的因素：</p><ul><li>FCFS</li><li>在内存限制的情况下，尽可能地增大batch_size以增大吞吐量</li></ul><p><img src="/posts_img/Orca/7.png?50"></p><p>通过流水线并行，迭代级调度可以消除bubble的影响</p><p><img src="/posts_img/Orca/8.png?50"></p><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>ORCA 使用了 <strong>层内并行</strong> 和 <strong>层间并行</strong> 模型来将大规模模型分布在多个 GPU 上。通过采用 GPU 分区，该系统可以扩展到拥有数百亿参数的 Transformer 模型（如 GPT-3 175B 及更大）。</p><table><thead><tr><th align="center"><img src="/posts_img/Orca/5.png" alt="Image 1"></th><th align="center"><img src="/posts_img/Orca/6.png" alt="Image 2"></th></tr></thead></table><p>该架构主要包括：</p><ul><li><strong>调度器</strong>：根据调度算法选择批。</li><li><strong>引擎主控</strong>：管理迭代级调度，并将任务分发到 GPU。</li><li><strong>工作单元</strong>：每个工作单元负责模型的一部分，可以在多个 GPU 之间处理请求。</li><li><strong>注意力 K&#x2F;V 管理器</strong>：管理和维护跨迭代的注意力键值对，允许其他操作进行选择性批处理。</li></ul><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><h2 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h2><p>ORCA 与 NVIDIA 的 FasterTransformer 进行了对比测试，测试的模型规模达到了 3410 亿参数（如 GPT-3）。实验结果显示：</p><ul><li>吞吐量提升 <strong>36.9</strong> 倍，并且延迟保持相同。</li><li>由于迭代级调度，<strong>排队时间</strong> 显著减少，从而加快响应速度。</li><li>在不同的批处理规模和工作负载下，ORCA 显著优于传统系统。</li></ul><p><img src="/posts_img/Orca/9.png?50"></p><h1 id="先前工作和讨论"><a href="#先前工作和讨论" class="headerlink" title="先前工作和讨论"></a>先前工作和讨论</h1><h2 id="BatchMaker-系统"><a href="#BatchMaker-系统" class="headerlink" title="BatchMaker 系统"></a>BatchMaker 系统</h2><p>BatchMaker 是一个用于 RNN（循环神经网络）的服务系统，能够以 RNN 单元的粒度进行调度和批处理。由于 RNN 每个单元执行相同的计算，因此可以将相同的单元进行批处理，即使它们处于不同的位置（即不同的词元索引）。这种灵活性允许新到达的请求加入当前正在执行的批次，而不需要等待整个批次完成。</p><p>然而，BatchMaker 无法对 Transformer 模型进行同样的批处理，因为 Transformer 中每个单元（如注意力机制中的键值对）随词元索引变化而不同。因此，BatchMaker 在处理不同词元时无法实现批处理，导致处理大多是串行的，并且无法支持大规模模型的并行训练。</p><h2 id="ORCA-的改进"><a href="#ORCA-的改进" class="headerlink" title="ORCA 的改进"></a>ORCA 的改进</h2><p>相比于 BatchMaker，ORCA 的设计基于最大化每次模型参数读取时的计算效率。这是因为对于大模型来说，从 GPU 全局内存读取参数是影响整体执行时间的主要瓶颈。ORCA 通过迭代级调度和选择性批处理来处理所有已准备好的词元，无论它们是否可以被批处理（如 Attention 操作不能被批处理）。</p><h2 id="针对-Transformer-模型的专用推理引擎"><a href="#针对-Transformer-模型的专用推理引擎" class="headerlink" title="针对 Transformer 模型的专用推理引擎"></a>针对 Transformer 模型的专用推理引擎</h2><p>Transformer 模型的性能促使了专门的推理系统的发展，如 FasterTransformer、LightSeq、TurboTransformers 等。这些系统主要作为现有服务系统（如 Triton 或 TensorFlow Serving）的后端推理引擎，仍采用传统的请求级调度。相比之下，ORCA 通过引入更细粒度的调度机制来提升效率。</p><h2 id="服务系统与推理引擎的接口"><a href="#服务系统与推理引擎的接口" class="headerlink" title="服务系统与推理引擎的接口"></a>服务系统与推理引擎的接口</h2><p>当前的通用服务系统如 Triton 和 Clipper 提供了抽象层来处理客户端请求，并调度底层的推理引擎。虽然这种设计分离了服务层和执行层的实现，但在处理像 GPT 这种多迭代特性的模型时存在局限性。ORCA 通过紧密集成调度器与引擎，简化了迭代级调度和选择性批处理的应用。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>ORCA 在处理 Transformer 模型的多次迭代和自回归特性方面显示出显著的改进。<strong>迭代级调度</strong> 和 <strong>选择性批处理</strong> 策略使 ORCA 能够高效处理大规模生成模型，提升了系统的性能和响应速度。</p>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOE基础介绍</title>
      <link href="/2024/09/13/MOE-Intro/"/>
      <url>/2024/09/13/MOE-Intro/</url>
      
        <content type="html"><![CDATA[<p>MOE重要性：坊间一直流传GPT-4是MoE模型<br>本文主要参考自：<a href="https://huggingface.co/blog/zh/moe">https://huggingface.co/blog/zh/moe</a></p><h1 id="什么是MOE"><a href="#什么是MOE" class="headerlink" title="什么是MOE"></a>什么是MOE</h1><p>基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p><p>● <strong>稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。</p><p>● <strong>门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。</p><p><img src="/posts_img/MOE-Intro/1.png"></p><div style="text-align: center;">  <a href="https://arxiv.org/pdf/1701.06538">Outrageously Large Neural Network 论文中的 MoE layer</a></div><p><img src="/posts_img/MOE-Intro/2.png"></p><div style="text-align: center;">  <a href="https://arxiv.org/pdf/2101.03961">Switch Transformers paper 论文中的 MoE layer</a></div><h1 id="混合专家模型（MoEs）简短总结"><a href="#混合专家模型（MoEs）简短总结" class="headerlink" title="混合专家模型（MoEs）简短总结"></a>混合专家模型（MoEs）简短总结</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>● 与稠密模型相比，<strong>预训练速度更快</strong><br>● 与具有相同参数数量的模型相比，具有更快的 <strong>推理速度</strong><br>● 与具有相同激活参数的稠密模型模型相比，具有更高的 <strong>推理精度</strong></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>● 需要 <strong>大量显存</strong>，因为所有专家系统都需要加载到内存中（现有offload技术）<br>● 在 <strong>微调方面</strong> 存在诸多挑战，但 <a href="https://arxiv.org/pdf/2305.14705">近期的研究</a> 表明，对混合专家模型进行指令调优具有很大的潜力<br>● 不同的专家倾向于专注于不同的 <strong>语义</strong>，而不是特定 <strong>领域</strong><br>● 由于设备间需要传递数据，网络带宽常常成为性能瓶颈，应采取适当的并行化策略（3D并行+专家并行）</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)<br>[2] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022)</p>]]></content>
      
      
      <categories>
          
          <category> Intro </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> MOE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOE利用Offload进行高效推理</title>
      <link href="/2024/09/12/MOE-Offloading/"/>
      <url>/2024/09/12/MOE-Offloading/</url>
      
        <content type="html"><![CDATA[<p>这篇文章提出了如何在资源受限的消费级硬件上高效地运行稀疏专家混合（MoE）语言模型的方法。将Mixtral-8x7B这个需要100G以上算力才能部署的模型在12G显存+11G内存的组合下跑出来。<br>论文：<a href="https://arxiv.org/abs/2312.17238">https://arxiv.org/abs/2312.17238</a><br>Colab代码：<a href="https://colab.research.google.com/drive/1ZkC0k487oBEF19R8_9nq2MSHFyQ6OspG?usp=drive_link">https://colab.research.google.com/drive/1ZkC0k487oBEF19R8_9nq2MSHFyQ6OspG?usp=drive_link</a></p><h1 id="引言与背景"><a href="#引言与背景" class="headerlink" title="引言与背景"></a>引言与背景</h1><p>论文的引言部分介绍了大规模预训练语言模型（LLMs）在自然语言处理领域的重要性。这些模型如GPT-3、GPT-4以及其他开放访问的LLMs（如LLaMA、Falcon、BLOOM等）推动了语言技术的迅猛发展。然而，LLMs 的庞大参数量使得它们的推理成本极高，通常需要高端的GPU设备才能运行，限制了它们在普通硬件上的使用。<br>为了缓解这个问题，稀疏的专家混合（MoE）模型被提出。MoE通过只激活模型中的一部分“专家”来计算每个输入，从而提高了计算效率。然而，MoE模型的规模依然庞大，尤其是在需要多GPU的环境下。因此，如何在消费级硬件上运行这些模型是一个重要的研究问题。</p><h1 id="三大解决策略"><a href="#三大解决策略" class="headerlink" title="三大解决策略"></a>三大解决策略</h1><p>Mixtral-8x7B模型中的总参数为46.7亿，专家构成45.1亿（96.6%），在内存受限的情况下，减少专家切换时GPU与RAM之间的数据传输对MoE模型进行推理很关键。作者主要提出通过LRU缓存（LRU caching）和专家的推测性提前加载（Speculative Expert Loading）来减少GPU与RAM之间的数据传输，从而加速推理过程。关键创新点包括：</p><ol><li><p>LRU缓存专家重用模式：MoE模型在处理连续的token时，某些专家会被频繁地重用。因此，作者设计了一种LRU缓存机制，利用这种专家重用的规律来减少GPU-RAM之间的通信开销。<br><img src="/posts_img/MOE-Offloading/1.png"></p></li><li><p>推测性专家加载：由于推理过程中无法提前确定下一层需要加载的专家，因此作者提出了一种基于推测的加载机制，通过对前一层的隐藏状态应用下一层的门控函数来猜测即将需要的专家（可能是因为有residual的原因）。这种机制在推测正确时，下一层的计算可以立即开始，显著减少了推理延迟。</p></li><li><p>混合量化技术：在模型压缩方面，作者使用了一种半二次量化（Half Quadratic Quantization, HQQ）的方法对专家层进行更高的压缩，同时保持其他层的较高精度。这种量化策略有效减少了模型大小，并保持了较好的推理性能。</p></li></ol><h1 id="实验与评估"><a href="#实验与评估" class="headerlink" title="实验与评估"></a>实验与评估</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>用到的模型：Mixtral 8x7B ，一个主流的MOE模型，在大多数基准测试中优于或等价于Llama2 70B, GPT3.5，且推理速度比Llama2 70B快六倍！<br>Mixtral 8x7B 是decoder-only model, 其中 FFN 从8个不同的参数组（专家）中进行挑选，在每一层，每个token, router network 都会选2个组来进行生成并进行组合：</p><ol><li>支持32K上下文</li><li>支持英语，法语，意大利语，德语，西班牙语（中文支持很差）</li><li>在代码生成上很强</li><li>能被微调成一个高分（MT-Bench）的 instruction-following model</li></ol><h2 id="评估结论"><a href="#评估结论" class="headerlink" title="评估结论"></a>评估结论</h2><p>论文在不同硬件配置（如RTX 3060、T4等）下对提出的方法进行了详尽的实验评估，得出了以下几个主要结论：</p><ol><li><p>专家缓存与推测加载的有效性：通过测试不同缓存大小和提前加载的专家数量，作者发现缓存命中率和推测加载的准确率显著提高了模型的推理速度。例如，在缓存大小为4时，缓存命中率可以达到约0.8，推测加载大小为2时，推测加载的准确率则可达到0.9以上。<br><img src="/posts_img/MOE-Offloading/2.png"></p></li><li><p>量化对模型性能的影响：通过对模型进行不同量化比特的测试，作者验证了在保持较好准确率的同时，量化可以有效减少模型大小。例如，使用2-bit量化时，模型的推理延迟显著降低，同时在WikiText2和C4数据集上的困惑度仅略有上升。</p></li><li><p>实际推理性能：在使用完整的算法时，消费级硬件上如RTX 3060和T4可以达到每秒生成2-3个token的性能，远远优于直接在设备内存不足的情况下推理时的性能表现。<br><img src="/posts_img/MOE-Offloading/3.png"></p></li></ol><h1 id="结论与未来工作"><a href="#结论与未来工作" class="headerlink" title="结论与未来工作"></a>结论与未来工作</h1><p>论文总结了该方法在推理速度上相较于传统的加载方式有显著提高，尤其是在消费级硬件和免费云平台（如Google Colab）上，使得大规模稀疏MoE模型的使用更加广泛化。未来的研究方向可能包括进一步优化专家预测加载算法，探索其他的推理加速方法。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇论文解决了大规模稀疏专家模型在推理时的硬件瓶颈问题，提出了一种通过专家缓存与预测加载来优化推理速度的方案，并使用混合量化技术在保证准确率的同时大幅减少了模型大小和推理时间。对于希望在低端硬件上使用大规模语言模型的研究人员来说，本文的贡献提供了一个具有实用价值的解决方案。</p><h1 id="Ideas"><a href="#Ideas" class="headerlink" title="Ideas"></a>Ideas</h1><ol><li>“Note that out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%).” 专家参数占了主导位置，这种异构型能否用于边缘-云端计算，隐私保护等</li><li>对于多用户多对话的在线推理服务系统，采取批量、并行的策略合理使用experts参数，增加吞吐、降低延迟等</li><li>将这种offload方法引入到训练过程中，可以显著扩大模型或数据集的规模</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper Report </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> MOE </tag>
            
            <tag> Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/09/12/hello-world/"/>
      <url>/2024/09/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><p>博客搭建参考文章：<a href="https://blog.csdn.net/mjh1667002013/article/details/129290903">【Hexo】Hexo搭建Butterfly主题并快速美化_hexo butterfly-CSDN博客</a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
