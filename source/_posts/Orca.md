---
layout: fast
title: Orca-大模型推理系统开山之作
date: 2024-09-14 12:24:22
summary: 
cover: posts_img/Orca/cover.png
categories: 
  - Paper Report
tags: 
  - LLM
  - Inference
---


本文提出了一种新的分布式服务系统 ORCA，针对大规模 Transformer 模型的自回归生成任务，解决了现有推理服务系统在多迭代特性任务上表现不佳的问题。ORCA 通过引入 **迭代级调度** 和 **选择性批处理** 两项技术，实现了更灵活高效的调度。实验结果表明，在处理 GPT-3 175B 模型时，ORCA 在保持相同延迟的情况下，吞吐量较 NVIDIA FasterTransformer 提升了 36.9 倍。

论文：(OSDI 2022)[ORCA: A Distributed Serving System for Transformer-Based Generative Models](https://www.usenix.org/conference/osdi22/presentation/yu)

# 引言
本文讨论了在服务大规模 Transformer 模型时面临的挑战，特别是用于生成任务的模型，如语言生成、代码生成等。典型的例子包括 GPT-3 这样的模型。随着对模型需求的不断增长，低延迟和高吞吐量成为推理系统的目标，早期通过使用如Triton和FasterTransformer的组合来部署服务，Triton主要负责将多个客户端请求分组到一个批中，而FasterTransformer作为模型推理进行优化的执行引擎从Triton接收该批，并以批处理的方式进行推理过程。这些模型通过一种多次迭代的自回归方式处理文本（即每次生成一个词元），这给现有系统带来了特定的挑战。

<!-- <p align="center">
  <img src="posts_img/Orca/1.png" alt="Description of image" width="700" />
</p> -->
![](posts_img/Orca/1.png?50)

# 问题陈述
现有的服务基于 Transformer 的生成模型的系统存在以下几个关键低效之处：

- 批调度：一个批次中已完成的请求必须等待整个批次完成，导致延迟增加。
- 排队延迟：新请求在当前批次完成之前无法处理。
- 多次迭代开销：由于每个序列中的词元必须通过迭代处理，同一模型需要多次调用来完成单个推理请求，这影响了吞吐量。

![](posts_img/Orca/2.png?50)


注意，在语言模型的训练中不会出现提前完成和延迟加入请求的问题；训练过程通过使用teacher forcing technique在一次迭代中完成对整个批次的处理。


# 解决方案
作者提出了 ORCA，一个优化大规模 Transformer 模型的分布式服务系统。ORCA 引入了两个关键技术：

## 迭代级调度
与其对整个请求进行批处理，ORCA 对每次迭代进行调度：

![](posts_img/Orca/3.png?50)

- 更快返回已完成的请求，减少延迟。
- 在当前迭代后立即处理新到的请求，允许更快的响应。

## 选择性批处理
针对上述所提到的迭代级调度的方法，在一个批中的请求有以下情况：(1) 所处阶段不同，prefill / decode。（2）序列长度不同。调度器希望同时存在这两个符合批处理条件的请求才能执行批处理，随着批处理大小的增加，这种可能性进一步呈指数级下降，因此使用大批处理大小来提高吞吐量而不影响延迟是困难的。

![](posts_img/Orca/4.png?100x)

针对此问题，作者提出的选择性批处理方法仅将批处理应用于特定操作（例如矩阵乘法），而对于无法批处理的操作（如注意力机制，输入张量大小可变的操作）则单独处理，这种选择性方法提升了效率，而不会增加额外的计算成本。

# 调度策略
本文中调度算法考虑的因素：
- FCFS
- 在内存限制的情况下，尽可能地增大batch_size以增大吞吐量

![](posts_img/Orca/7.png?50)

通过流水线并行，迭代级调度可以消除bubble的影响

![](posts_img/Orca/8.png?50)

# 系统架构
ORCA 使用了 **层内并行** 和 **层间并行** 模型来将大规模模型分布在多个 GPU 上。通过采用 GPU 分区，该系统可以扩展到拥有数百亿参数的 Transformer 模型（如 GPT-3 175B 及更大）。

| ![Image 1](posts_img/Orca/5.png) | ![Image 2](posts_img/Orca/6.png) |
|:------------------------------:|:------------------------------:|


该架构主要包括：
- **调度器**：根据调度算法选择批。
- **引擎主控**：管理迭代级调度，并将任务分发到 GPU。
- **工作单元**：每个工作单元负责模型的一部分，可以在多个 GPU 之间处理请求。
- **注意力 K/V 管理器**：管理和维护跨迭代的注意力键值对，允许其他操作进行选择性批处理。

# 评估

## 基准测试
ORCA 与 NVIDIA 的 FasterTransformer 进行了对比测试，测试的模型规模达到了 3410 亿参数（如 GPT-3）。实验结果显示：
- 吞吐量提升 **36.9** 倍，并且延迟保持相同。
- 由于迭代级调度，**排队时间** 显著减少，从而加快响应速度。
- 在不同的批处理规模和工作负载下，ORCA 显著优于传统系统。

![](posts_img/Orca/9.png?50)

# 先前工作和讨论
## BatchMaker 系统
BatchMaker 是一个用于 RNN（循环神经网络）的服务系统，能够以 RNN 单元的粒度进行调度和批处理。由于 RNN 每个单元执行相同的计算，因此可以将相同的单元进行批处理，即使它们处于不同的位置（即不同的词元索引）。这种灵活性允许新到达的请求加入当前正在执行的批次，而不需要等待整个批次完成。

然而，BatchMaker 无法对 Transformer 模型进行同样的批处理，因为 Transformer 中每个单元（如注意力机制中的键值对）随词元索引变化而不同。因此，BatchMaker 在处理不同词元时无法实现批处理，导致处理大多是串行的，并且无法支持大规模模型的并行训练。

## ORCA 的改进
相比于 BatchMaker，ORCA 的设计基于最大化每次模型参数读取时的计算效率。这是因为对于大模型来说，从 GPU 全局内存读取参数是影响整体执行时间的主要瓶颈。ORCA 通过迭代级调度和选择性批处理来处理所有已准备好的词元，无论它们是否可以被批处理（如 Attention 操作不能被批处理）。

## 针对 Transformer 模型的专用推理引擎
Transformer 模型的性能促使了专门的推理系统的发展，如 FasterTransformer、LightSeq、TurboTransformers 等。这些系统主要作为现有服务系统（如 Triton 或 TensorFlow Serving）的后端推理引擎，仍采用传统的请求级调度。相比之下，ORCA 通过引入更细粒度的调度机制来提升效率。

## 服务系统与推理引擎的接口
当前的通用服务系统如 Triton 和 Clipper 提供了抽象层来处理客户端请求，并调度底层的推理引擎。虽然这种设计分离了服务层和执行层的实现，但在处理像 GPT 这种多迭代特性的模型时存在局限性。ORCA 通过紧密集成调度器与引擎，简化了迭代级调度和选择性批处理的应用。

# 结论
ORCA 在处理 Transformer 模型的多次迭代和自回归特性方面显示出显著的改进。**迭代级调度** 和 **选择性批处理** 策略使 ORCA 能够高效处理大规模生成模型，提升了系统的性能和响应速度。
